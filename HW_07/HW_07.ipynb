{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ca41e2",
   "metadata": {},
   "source": [
    "# Домашнее задание № 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0588a6",
   "metadata": {},
   "source": [
    "### Задание 1 Реализовать алгоритм Леска и проверить его на реальном датасете (8 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5283f",
   "metadata": {},
   "source": [
    "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное.\n",
    "\n",
    "Реализуйте его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91da1f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668e6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    \n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    \n",
    "    # лемматизируем контекст (предложение)\n",
    "    sentence_lemmatized = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    \n",
    "    # находим в wordnet синсеты для слова\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    # находим пересечения значений синсетов с контекстом \n",
    "    for i, synset in enumerate(synsets):\n",
    "        definition = synset.definition().lower().split()\n",
    "        definition_lemmatized = [lemmatizer.lemmatize(word) for word in definition]\n",
    "        overlap = len(list(set(sentence_lemmatized) & set(definition_lemmatized)))\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap, bestsense = overlap, i\n",
    "       \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5f53b",
   "metadata": {},
   "source": [
    "Чтобы проверить работу алгоритма, напишем функцию для извлечения слов и их контекстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84d54188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_context(text, tokenizer=True, window=5):\n",
    "    word2context = []\n",
    "    if tokenizer:\n",
    "        text_tokenized = [word.lower() for word in word_tokenize(text)]\n",
    "    else:\n",
    "        text_tokenized = text\n",
    "    for i, word in enumerate(text_tokenized):\n",
    "        word2context.append((word, text_tokenized[max(0, i-window):i] + text_tokenized[i+1:i+window]))\n",
    "    return word2context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a514d8a6",
   "metadata": {},
   "source": [
    "Проверим на каком-нибудь предложении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e526f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Walk through the mountain gorge and rest on the river bank.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f8db3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: walk\n",
      "Sense in WordNet: the act of traveling by foot\n",
      "\n",
      "\n",
      "Word: through\n",
      "Sense in WordNet: over the whole distance\n",
      "\n",
      "\n",
      "Word: the\n",
      "Sense in WordNet: out of dictionary\n",
      "\n",
      "\n",
      "Word: mountain\n",
      "Sense in WordNet: a land mass that projects well above its surroundings; higher than a hill\n",
      "\n",
      "\n",
      "Word: gorge\n",
      "Sense in WordNet: the passage between the pharynx and the stomach\n",
      "\n",
      "\n",
      "Word: and\n",
      "Sense in WordNet: out of dictionary\n",
      "\n",
      "\n",
      "Word: rest\n",
      "Sense in WordNet: euphemisms for death (based on an analogy between lying in a bed and in a tomb)\n",
      "\n",
      "\n",
      "Word: on\n",
      "Sense in WordNet: in operation or operational\n",
      "\n",
      "\n",
      "Word: the\n",
      "Sense in WordNet: out of dictionary\n",
      "\n",
      "\n",
      "Word: river\n",
      "Sense in WordNet: a large natural stream of water (larger than a creek)\n",
      "\n",
      "\n",
      "Word: bank\n",
      "Sense in WordNet: a financial institution that accepts deposits and channels the money into lending activities\n",
      "\n",
      "\n",
      "Word: .\n",
      "Sense in WordNet: out of dictionary\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word2context in get_words_in_context(text):\n",
    "    word, context = word2context\n",
    "    sense = lesk(word, context)\n",
    "    print('Word:', word)\n",
    "    if wn.synsets(word):\n",
    "        print('Sense in WordNet:', wn.synsets(word)[sense].definition())\n",
    "    else:  \n",
    "        print('Sense in WordNet: out of dictionary')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38251eeb",
   "metadata": {},
   "source": [
    "Алгоритм не справился с многозначными словами gorge, rest, bank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4309c",
   "metadata": {},
   "source": [
    "**Проверьте насколько хорошо работает такой метод на датасете из семинара**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa7ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf01fee",
   "metadata": {},
   "source": [
    "Корпус состоит из предложений, где у каждого слова три поля - значение, лемма и само слово. Значение пустое, когда слово однозначное, а у многозначных слов стоит тэг вида **'long%3:00:02::'** Это тэг wordnet'ного формата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87b02c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'how', 'How'],\n",
       " ['long%3:00:02::', 'long', 'long'],\n",
       " ['', 'have', 'has'],\n",
       " ['', 'it', 'it'],\n",
       " ['be%2:42:03::', 'be', 'been'],\n",
       " ['', 'since', 'since'],\n",
       " ['', 'you', 'you'],\n",
       " ['review%2:31:00::', 'review', 'reviewed'],\n",
       " ['', 'the', 'the'],\n",
       " ['objective%1:09:00::', 'objective', 'objectives'],\n",
       " ['', 'of', 'of'],\n",
       " ['', 'you', 'your'],\n",
       " ['benefit%1:21:00::', 'benefit', 'benefit'],\n",
       " ['', 'and', 'and'],\n",
       " ['service%1:04:07::', 'service', 'service'],\n",
       " ['program%1:09:01::', 'program', 'program'],\n",
       " ['', '?', '?']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_wsd[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799b8806",
   "metadata": {},
   "source": [
    "**Вам нужно для каждого многозначного слова (т.е. у него есть тэг в первом поле) с помощью алгоритма Леска предсказать нужный синсет и сравнить с правильным. Посчитайте процент правильных предсказаний (accuracy).**\n",
    "\n",
    "Если считается слишком долго, возьмите поменьше предложений (например, только тысячу)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66cfb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.37558198180173646\n"
     ]
    }
   ],
   "source": [
    "all_words = 0\n",
    "sim_words = 0\n",
    "for sent in corpus_wsd:\n",
    "    try:\n",
    "        sent_tokens = [word[2].lower() for word in sent]\n",
    "        for i, word in enumerate(sent):\n",
    "            if word[0]:\n",
    "                all_words += 1\n",
    "                context = sent_tokens[max(0, i-5):i] + sent_tokens[i+1:i+5]\n",
    "                sense = lesk(word[1], context)\n",
    "                # в датасете все многозначные слова присутствует в wordnet,\n",
    "                # но на случай, если какого-то слова нет в словаре, добавим условие:\n",
    "                if wn.synsets(word[1]):\n",
    "                    if wn.synsets(word[1])[sense] == wn.lemma_from_key(word[0]).synset():\n",
    "                        sim_words += 1\n",
    "    # для исключения предложений из датасета, где у слов нет тэгов\n",
    "    except IndexError:\n",
    "        pass\n",
    "print('Accuracy score:', sim_words/all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85104f",
   "metadata": {},
   "source": [
    "Возможно, на результат могло повлиять наличие стоп-слов. Попробуем не учитывать их при сопоставлении контекста слова и его определения. Для этого добавим условие в функцию lesk():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67ca8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def lesk_without_stopwords(word, sentence):\n",
    "    \n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    \n",
    "    # добавляем условие 'if word not in stopwords'\n",
    "    sentence_lemmatized = [lemmatizer.lemmatize(word) for word in sentence if word not in stopwords]\n",
    "\n",
    "    synsets = wn.synsets(word)\n",
    "\n",
    "    for i, synset in enumerate(synsets):\n",
    "        definition = synset.definition().lower().split()\n",
    "        # добавляем условие 'if word not in stopwords'\n",
    "        definition_lemmatized = [lemmatizer.lemmatize(word) for word in definition if word not in stopwords]\n",
    "        overlap = len(list(set(sentence_lemmatized) & set(definition_lemmatized)))\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap, bestsense = overlap, i\n",
    "       \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72a1463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.4687490882111432\n"
     ]
    }
   ],
   "source": [
    "all_words = 0\n",
    "sim_words = 0\n",
    "for sent in corpus_wsd:\n",
    "    try:\n",
    "        sent_tokens = [word[2].lower() for word in sent]\n",
    "        for i, word in enumerate(sent):\n",
    "            if word[0]:\n",
    "                all_words += 1\n",
    "                context = sent_tokens[max(0, i-5):i] + sent_tokens[i+1:i+5]\n",
    "                num = lesk_without_stopwords(word[1], context)\n",
    "                if wn.synsets(word[1])[num] == wn.lemma_from_key(word[0]).synset():\n",
    "                    sim_words += 1\n",
    "    except IndexError:\n",
    "        pass\n",
    "print('Accuracy score:', sim_words/all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6aaf74",
   "metadata": {},
   "source": [
    "Уже выглядит лучше. Посмотрим, как теперь разбирается наше предложение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7943a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: walk\n",
      "Sense in WordNet: the act of traveling by foot\n",
      "\n",
      "\n",
      "Word: through\n",
      "Sense in WordNet: having finished or arrived at completion\n",
      "\n",
      "\n",
      "Word: the\n",
      "Sense in WordNet: out of dictionary\n",
      "\n",
      "\n",
      "Word: mountain\n",
      "Sense in WordNet: a land mass that projects well above its surroundings; higher than a hill\n",
      "\n",
      "\n",
      "Word: gorge\n",
      "Sense in WordNet: a deep ravine (usually with a river running through it)\n",
      "\n",
      "\n",
      "Word: and\n",
      "Sense in WordNet: out of dictionary\n",
      "\n",
      "\n",
      "Word: rest\n",
      "Sense in WordNet: something left after other parts have been taken away\n",
      "\n",
      "\n",
      "Word: on\n",
      "Sense in WordNet: in operation or operational\n",
      "\n",
      "\n",
      "Word: the\n",
      "Sense in WordNet: out of dictionary\n",
      "\n",
      "\n",
      "Word: river\n",
      "Sense in WordNet: a large natural stream of water (larger than a creek)\n",
      "\n",
      "\n",
      "Word: bank\n",
      "Sense in WordNet: sloping land (especially the slope beside a body of water)\n",
      "\n",
      "\n",
      "Word: .\n",
      "Sense in WordNet: out of dictionary\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word2context in get_words_in_context(text):\n",
    "    word, context = word2context\n",
    "    sense = lesk_without_stopwords(word, context)\n",
    "    print('Word:', word)\n",
    "    if wn.synsets(word):\n",
    "        print('Sense in WordNet:', wn.synsets(word)[sense].definition())\n",
    "    else:  \n",
    "        print('Sense in WordNet: out of dictionary')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec8d8b9",
   "metadata": {},
   "source": [
    "Теперь для слов gorge и bank значения определены верно. Для слова rest значение изменилось, но осталось неправильным. Попробуем посмотреть на все значения этого слова в WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba1557f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something left after other parts have been taken away\n",
      "freedom from activity (work or strain or responsibility)\n",
      "a pause for relaxation\n",
      "a state of inaction\n",
      "euphemisms for death (based on an analogy between lying in a bed and in a tomb)\n",
      "a support on which things can be put\n",
      "a musical notation indicating a silence of a specified duration\n",
      "not move; be in a resting position\n",
      "take a short break from one's activities in order to relax\n",
      "give a rest to\n",
      "have a place in relation to something else\n",
      "be at rest\n",
      "stay the same; remain in a certain state\n",
      "be inherent or innate in\n",
      "put something in a resting position, as for support or steadying\n",
      "sit, as on a branch\n",
      "rest on or as if on a pillow\n",
      "be inactive, refrain from acting\n"
     ]
    }
   ],
   "source": [
    "for sense in wn.synsets('rest'):\n",
    "    print(sense.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67efa113",
   "metadata": {},
   "source": [
    "Заметно, что некоторые значения описываются 3-4 словами, что, конечно, недостаточно для совпадения с контекстом. Это можно отнести к очевидным минусам метода Леска (из вариантов улучшений метода -- использовать словари синонимов, учитывать н-граммы, чтобы находить однокоренные слова и т.п.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b45d4",
   "metadata": {},
   "source": [
    "### Задание 2* (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ef129",
   "metadata": {},
   "source": [
    "В семинаре для WSI на данных Диалога использовался только Fastext. Попробуйте заменить его на адаграм (обучите свою модель или используйте предобученную out.pkl или https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib), а также поэкспериментируйте с разными алгоритмами кластеризации и их параметрами (можно использовать только те алгоритмы, которые обучаются достаточно быстро).\n",
    "\n",
    "Для каждого эксперимента рассчитайте ARI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20e75a",
   "metadata": {},
   "source": [
    "Для эмбеддингов будем использовать [предобученную модель adagram](https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58c50c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import adagram\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619c84c",
   "metadata": {},
   "source": [
    "Загружаем датасет, создаем функцию для препроцессинга контекстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ecbdd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_id</th>\n",
       "      <th>word</th>\n",
       "      <th>gold_sense_id</th>\n",
       "      <th>predict_sense_id</th>\n",
       "      <th>positions</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0-5, 339-344</td>\n",
       "      <td>замок владимира мономаха в любече . многочисле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11-16, 17-22, 188-193</td>\n",
       "      <td>шильонский замок замок шильйон ( ) , известный...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>299-304</td>\n",
       "      <td>проведения архитектурно - археологических рабо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111-116</td>\n",
       "      <td>топи с . , л . белокуров легенда о завещании м...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>замок</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134-139, 262-267</td>\n",
       "      <td>великий князь литовский гедимин после успешной...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   context_id   word  gold_sense_id  predict_sense_id              positions  \\\n",
       "0           1  замок              1               NaN           0-5, 339-344   \n",
       "1           2  замок              1               NaN  11-16, 17-22, 188-193   \n",
       "2           3  замок              1               NaN                299-304   \n",
       "3           4  замок              1               NaN                111-116   \n",
       "4           5  замок              1               NaN       134-139, 262-267   \n",
       "\n",
       "                                             context  \n",
       "0  замок владимира мономаха в любече . многочисле...  \n",
       "1  шильонский замок замок шильйон ( ) , известный...  \n",
       "2  проведения архитектурно - археологических рабо...  \n",
       "3  топи с . , л . белокуров легенда о завещании м...  \n",
       "4  великий князь литовский гедимин после успешной...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb1f13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby('word')[['word', 'context', 'gold_sense_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5bf5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokenized = re.findall('[А-Яа-яёЁA-Za-z0-9-]+', text.lower())\n",
    "    lemmatized = [morph.parse(token)[0].normal_form for token in tokenized]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b666e",
   "metadata": {},
   "source": [
    "Посмотрим, для скольких значений слов приведены контексты (gold_sense_id). Эта информация понадобится для указания количества кластеров (например, в алгоритме KMeans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8e13235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "замок [1 2]\n",
      "лук [1 2]\n",
      "суда [1 2]\n",
      "бор [1 2]\n"
     ]
    }
   ],
   "source": [
    "for word in df.word.unique():\n",
    "    print(word, df[df.word == word].gold_sense_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517092a",
   "metadata": {},
   "source": [
    "Загружаем модель adagram и создадим функцию для получения усредненного эмбеддинга текста с использованием модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c17e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_adagram = adagram.VectorModel.load('model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6899c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# у готовой модели размерность векторов 300, поэтому dim=300\n",
    "\n",
    "def get_embedding_adagram(text, window, model=vm_adagram, dim=300):\n",
    "    \n",
    "    word2context = []\n",
    "    \n",
    "    for i in range(len(text)-1):\n",
    "        word = text[i]\n",
    "        context = text[max(0, i-window):i] + text[i+1:i+window]\n",
    "        word2context.append((word, context))\n",
    "        \n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    \n",
    "    for i, elem in enumerate(word2context):\n",
    "        word, context = elem\n",
    "        try:\n",
    "            sense = model.disambiguate(word, context).argmax()\n",
    "            vectors[i] = model.sense_vector(word, sense)\n",
    "        except (KeyError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6a6c0",
   "metadata": {},
   "source": [
    "Создадим функцию для кластеризации контекстов (с подсчетом метрики ARI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e39baaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(clusterizer, corpus=grouped_df):\n",
    "    \n",
    "    ARI = []\n",
    "    \n",
    "    for key, _ in corpus:\n",
    "        \n",
    "        texts = corpus.get_group(key)['context'].apply(preprocess)\n",
    "        X = np.zeros((len(texts), 300))\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            text = [word for word in text if word != key]\n",
    "            X[i] = get_embedding_adagram(text=text, window=5)\n",
    "            \n",
    "        clusterizer.fit(X)\n",
    "        labels = np.array(clusterizer.labels_)+1\n",
    "        \n",
    "        ARI.append(adjusted_rand_score(corpus.get_group(key)['gold_sense_id'], labels))\n",
    "        \n",
    "    return np.mean(ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513c4e6",
   "metadata": {},
   "source": [
    "Поэкспериментируем с разными алгоритмами кластеризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "057c8059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49803872540257665"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterize(KMeans(n_clusters=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20756f1b",
   "metadata": {},
   "source": [
    "KMeans даже с учетом явного указания количества кластеров показывает 0.49. Посмотрим, как работает другой алгоритм, где тоже можно задавать количество кластеров, -- AgglomerativeClustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59145cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49814828906349823"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterize(AgglomerativeClustering(n_clusters=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8c338",
   "metadata": {},
   "source": [
    "Результат почти не отличается. Теперь взглянем на алгоритм AffinityPropogation. Попробуем подобрать оптимальный параметр preference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40d6e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference: -20\n",
      "ARI score: 0.029354654102275248\n",
      "Preference: -15\n",
      "ARI score: 0.33150732871317123\n",
      "Preference: -10\n",
      "ARI score: 0.44789643381100785\n",
      "Preference: -5\n",
      "ARI score: 0.28799833320357526\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(-20, 0, 5):\n",
    "    print('Preference:', i)\n",
    "    print('ARI score:', clusterize(AffinityPropagation(damping=0.7, preference=i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caee153",
   "metadata": {},
   "source": [
    "При preference -10 ARI наилучший. Теперь попробуем подкрутить параметр damping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c511e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Damping: 0.5\n",
      "ARI score: 0.40338052911441835\n",
      "Damping: 0.6\n",
      "ARI score: 0.5395473972163697\n",
      "Damping: 0.7\n",
      "ARI score: 0.44789643381100785\n",
      "Damping: 0.7999999999999999\n",
      "ARI score: 0.4910811066157188\n"
     ]
    }
   ],
   "source": [
    "for i in np.arange(0.5, 0.9, 0.1):\n",
    "    print('Damping:', i)\n",
    "    print('ARI score:', clusterize(AffinityPropagation(damping=i, preference=-10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b932a",
   "metadata": {},
   "source": [
    "При параметрах preference=-10 и damping=0.6 AffinityPropagation работает лучше, чем остальные рассмотренные алгоритмы (ARI score равен 0.54)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
