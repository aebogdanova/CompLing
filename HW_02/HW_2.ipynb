{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3705663",
   "metadata": {},
   "source": [
    "# Домашнее задание № 2. Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf72d19",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045e99",
   "metadata": {},
   "source": [
    "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способо заменить её на кастомную токенизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4d453",
   "metadata": {},
   "source": [
    "Обучите векторайзер с дефолтной токенизацией и с токенизацией razdel.tokenize. Обучите классификатор с каждым из векторизаторов. Сравните метрики и выберете победителя. \n",
    "\n",
    "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129c4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from razdel import tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbc2365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скрипалей отравила Россия? Анализировать и думать пытаешься? Ватник что ли?)\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                     comment  \\\n",
       "0                                                                                                                                                       Верблюдов-то за что? Дебилы, бл...\\n   \n",
       "1                                                                 Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n   \n",
       "2                                                                                                                                                                  Собаке - собачья смерть\\n   \n",
       "3  Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\\n   \n",
       "4                                                                  тебя не убедил 6-страничный пдф в том, что Скрипалей отравила Россия? Анализировать и думать пытаешься? Ватник что ли?)\\n   \n",
       "\n",
       "   toxic  \n",
       "0    1.0  \n",
       "1    1.0  \n",
       "2    1.0  \n",
       "3    1.0  \n",
       "4    1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14412, 2)\n",
      "0.0    0.66514\n",
      "1.0    0.33486\n",
      "Name: toxic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('labeled.csv')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(data.head(5))\n",
    "print(data.shape)\n",
    "print(data.toxic.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45235ca",
   "metadata": {},
   "source": [
    "Обучим классификатор с помощью алгоритма Decision Tree:\n",
    "- с векторизацией через TfidfVectorizer (дефолтная токенизация)\n",
    "- с векторизацией через TfidfVectorizer (токенизация через razdel.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1dd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "train, test = train_test_split(data, test_size=0.15, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38889dd3",
   "metadata": {},
   "source": [
    "Векторизуем тексты с помощью дефолтной токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc75729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12250, 62004) (2162, 62004)\n"
     ]
    }
   ],
   "source": [
    "#build feature vectors with default tokenization\n",
    "vectorizer_default = TfidfVectorizer()\n",
    "X_train = vectorizer_default.fit_transform(train.comment)\n",
    "X_test = vectorizer_default.transform(test.comment) \n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33650b46",
   "metadata": {},
   "source": [
    "Векторизуем тексты с помощью токенизации через razdel.tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78058a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom tokenizer function (razdel.tokenize)\n",
    "def razdel_tokenize(string):\n",
    "    string_tokenized = [token.text for token in list(tokenize(string))]\n",
    "    return string_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f87b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12250, 62912) (2162, 62912)\n"
     ]
    }
   ],
   "source": [
    "#build feature vectors with custom tokenization\n",
    "vectorizer_razdel = TfidfVectorizer(tokenizer=razdel_tokenize, token_pattern=None)\n",
    "X_train_razdel = vectorizer_razdel.fit_transform(train.comment)\n",
    "X_test_razdel = vectorizer_razdel.transform(test.comment) \n",
    "print(X_train_razdel.shape, X_test_razdel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f8fa5",
   "metadata": {},
   "source": [
    "Задаем целевые переменные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e328f0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12250,) (2162,)\n"
     ]
    }
   ],
   "source": [
    "#target values\n",
    "y_train = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bfd76",
   "metadata": {},
   "source": [
    "Обучаем классификатор на дефолтной токенизации, смотрим метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d184ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.80      0.80      1479\n",
      "         1.0       0.58      0.59      0.59       683\n",
      "\n",
      "    accuracy                           0.73      2162\n",
      "   macro avg       0.69      0.70      0.69      2162\n",
      "weighted avg       0.74      0.73      0.73      2162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#learning with default tokenization\n",
    "DTC = DecisionTreeClassifier()\n",
    "DTC.fit(X_train, y_train)\n",
    "\n",
    "#metrics\n",
    "DTC_preds = DTC.predict(X_test)\n",
    "print(classification_report(y_test, DTC_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a698717b",
   "metadata": {},
   "source": [
    "Обучаем классификатор на кастомной токенизации, смотрим метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06d12b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.81      0.81      1479\n",
      "         1.0       0.59      0.57      0.58       683\n",
      "\n",
      "    accuracy                           0.74      2162\n",
      "   macro avg       0.70      0.69      0.69      2162\n",
      "weighted avg       0.74      0.74      0.74      2162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#learning with custom tokenization\n",
    "DTC.fit(X_train_razdel, y_train)\n",
    "\n",
    "#metrics\n",
    "DTC_preds = DTC.predict(X_test_razdel)\n",
    "print(classification_report(y_test, DTC_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64d6dd",
   "metadata": {},
   "source": [
    "Метрики почти одинаковые, но заметно, что для класса токсичных комментариев токенизация через razdel.tokenize сработала чуть хуже: несмотря на то, что точность выше на 1 процент, полнота ниже на 2 процента (соответственно, и F1-мера проигрывает стандартной токенизации). Точность и полнота для нетоксичных комментариев почти одинаковая: у razdel.tokenize выше полнота, у стандартной токенизации -- точность. Если задача состоит в том, чтобы лучше определять токсичные комментарии, то имеет смысл воспользоваться стандартной токенизацией. Не исключено также, что при другом классификаторе соотношение метрик для стандартного и заданного вручную токенизатора будет отличаться."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffa9f76",
   "metadata": {},
   "source": [
    "## Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f358949",
   "metadata": {},
   "source": [
    "Преобразуйте таблицу с абсолютными частотностями в семинарской тетрадке в таблицу с tfidf значениями. (Таблица - https://i.ibb.co/r5Nc2HC/abs-bow.jpg) Формула tfidf есть в семинаре на картнике с пояснениями на английском. \n",
    "Считать нужно в питоне. Формат итоговой таблицы может быть любым, главное, чтобы был код и можно было воспроизвести вычисления. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5b50abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.ibb.co/r5Nc2HC/abs-bow.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"https://i.ibb.co/r5Nc2HC/abs-bow.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082179e0",
   "metadata": {},
   "source": [
    "Напишем функцию для подсчета TF-IDF. На вход функции подается список документов, на выходе -- матрица \"термин-документ\" и список токенов (для визуализации в виде датафрейма)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15091187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_counter(docs):\n",
    "    docs_tokenized= [re.findall('\\w+', doc) for doc in docs]   #simple tokenizer for documents\n",
    "    terms = list(set([term for doc in docs_tokenized for term in doc]))  #list of terms\n",
    "    matrix = np.zeros((len(docs), len(terms))) #set document-term matrix\n",
    "    for term in terms:\n",
    "        count = 0\n",
    "        for doc in docs_tokenized:\n",
    "            if term in doc:\n",
    "                count += 1\n",
    "        matrix[:,terms.index(term)] += count   #filling matrix with document frequency\n",
    "    matrix = np.log(len(docs)/matrix)    #counting inverse document frequency\n",
    "    for num, doc in enumerate(docs_tokenized):\n",
    "            counter = Counter(doc)\n",
    "            for term in terms:\n",
    "                matrix[num, terms.index(term)] *= counter[term]/len(doc)   #counting tf-idf\n",
    "    return matrix, terms        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c3448c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>я</th>\n",
       "      <th>не</th>\n",
       "      <th>он</th>\n",
       "      <th>и</th>\n",
       "      <th>ты</th>\n",
       "      <th>только</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>я и ты</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170275</td>\n",
       "      <td>0.30543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ты и я</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170275</td>\n",
       "      <td>0.30543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>я, я и только я</th>\n",
       "      <td>0.133886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102165</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.183258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>только не я</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.305430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>он</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        я        не        он         и       ты    только\n",
       "я и ты           0.074381  0.000000  0.000000  0.170275  0.30543  0.000000\n",
       "ты и я           0.074381  0.000000  0.000000  0.170275  0.30543  0.000000\n",
       "я, я и только я  0.133886  0.000000  0.000000  0.102165  0.00000  0.183258\n",
       "только не я      0.074381  0.536479  0.000000  0.000000  0.00000  0.305430\n",
       "он               0.000000  0.000000  1.609438  0.000000  0.00000  0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = ['я и ты', 'ты и я', 'я, я и только я', 'только не я', 'он']\n",
    "\n",
    "df = pd.DataFrame(tfidf_counter(texts)[0], index=texts, columns=tfidf_counter(texts)[1])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5bc8de",
   "metadata": {},
   "source": [
    "## Задание 3 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8961bbf",
   "metadata": {},
   "source": [
    "Обучите 2 любых разных классификатора из семинара. Предскажите токсичность для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов. Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46681ef",
   "metadata": {},
   "source": [
    "Требования к классификаторам:   \n",
    "а) один должен использовать CountVectorizer, другой TfidfVectorizer  \n",
    "б) у векторазера должны быть вручную заданы как минимум 5 параметров  \n",
    "в) у классификатора должно быть задано вручную как минимум 2 параметра  \n",
    "г)  f1 мера каждого из классификаторов должна быть минимум 0.75  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1850904",
   "metadata": {},
   "source": [
    "Сначала разобьем датасет на обучающую и тестовую выборки (для классификаторов нам необходимы одинаковые выборки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed77d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63ad538",
   "metadata": {},
   "source": [
    "Теперь векторизуем комментарии. В качестве настраиваемых параметров для векторайзеров в sklearn возьмем lowercase, tokenizer (вместе с token_pattern), min_df, max_df и max_feautures.\n",
    "\n",
    "Свой токенизатор нам нужен по следующим причинам:\n",
    "1. Стандартный токенизатор в sklearn-векторайзерах делит предложения на токены по регулярному выражению r”(?u)\\b\\w\\w+\\b”. Если использовать такой токенизатор, то слова, которые пишутся через дефис (например, \"Ростов-на-Дону\") разобьются на три токена. Для нашей задачи может быть важно сохранять такие слова.\n",
    "2. К специфике токсичных комментариев можно отнести слова, написанные полностью в верхнем регистре (как показатели экспрессивности высказываний), поэтому такие слова нам важно будет оставить \"как есть\" -- они могут быть значимыми признаками для моделей. \n",
    "3. Вместо стандартной токенизации по словам попробуем использовать стемминг -- он уменьшит количество признаков за счет удаления различных окончаний, но при этом сохранит лексические значения корня."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63680ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom tokenize function\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('russian')\n",
    "\n",
    "def tokenize_text(string):\n",
    "    string = re.findall(r'\\b\\w+-?\\w+\\b', string)\n",
    "    string_case= []\n",
    "    for token in string:\n",
    "        if token.isupper():\n",
    "            string_case.append(token)\n",
    "        else:\n",
    "            string_case.append(token.lower())\n",
    "    string_case = [stemmer.stem(token) for token in string_case]\n",
    "    return string_case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3838a640",
   "metadata": {},
   "source": [
    "При подборе параметров min_df и max_df выяснилось следующее:\n",
    "1. Параметр max_df не имеет смысла брать большой: в датасете нет токенов, которые встречались бы более, чем в 30% документов (при max_df=0.3 из словаря токенов удаляется только одно слово). Оставим параметр на значении 0.05 -- тогда из датасета удаляется 321  слово (то есть мы не берем в расчет слова, которые встречаются в каждом двадцатом документе).\n",
    "2. По параметру min_df можно сделать вывод о том, что в комментариях довольно разнообразная лексика. С параметром min_df=2 уже игнорируется чуть более половины токенов, что значит, что в каждом втором комментарии встречается уникальное слово (и это при том, что токенами считаются не полноценные слова, а только основы). Чтобы нам хватило признаков для обучения, оставим этот параметр на значении 2.\n",
    "3. При значительном уменьшении параметра max_features метрики становятся хуже (скорее всего, модели не хватает данных для обучения), поэтому этот параметр оставим приближенным к размеру обучающей выборки (12000).\n",
    "\n",
    "Так как мы используем свой токенизатор, параметры token_pattern и lowercase должны быть со значением False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd916f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12970, 12000) (1442, 12000)\n"
     ]
    }
   ],
   "source": [
    "#build feature vectors with CountVectorizer\n",
    "count_vectorizer = CountVectorizer(lowercase=False, tokenizer=tokenize_text, token_pattern=False, max_df=0.05, min_df=2, max_features=12000)\n",
    "X_train_count = count_vectorizer.fit_transform(train.comment)\n",
    "X_test_count = count_vectorizer.transform(test.comment) \n",
    "print(X_train_count.shape, X_test_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c92d0986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12970, 12000) (1442, 12000)\n"
     ]
    }
   ],
   "source": [
    "#build feature vectors with TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=False, tokenizer=tokenize_text, token_pattern=False, max_df=0.05, min_df=2, max_features=12000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train.comment)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test.comment) \n",
    "print(X_train_tfidf.shape, X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff20596",
   "metadata": {},
   "source": [
    "Задаем целевые переменные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fda9da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12970,) (1442,)\n"
     ]
    }
   ],
   "source": [
    "#target values\n",
    "y_train = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a860bc99",
   "metadata": {},
   "source": [
    "Векторы частотности будем использовать для обучения наивного байесовского классификатора (он хорошо подходит для классификации дискретных признаков). В качестве вручную заданных параметров будем использовать alpha и class_prior.\n",
    "\n",
    "Для того, чтобы подобрать оптимальный параметр alpha, используем np.arrange из возможных значений alpha и посмотрим, при каком из них классификатор дает высокое значение F1-меры для тестовой выборки (заодно посмотрим на соответствующие значения accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8868c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a list of alpha (from 0.01 to 100 with step=0.1)\n",
    "list_alpha = np.arange(0.01, 100, 0.1)\n",
    "#set lists for scores\n",
    "score_train = np.zeros(len(list_alpha))\n",
    "score_test = np.zeros(len(list_alpha))\n",
    "f1_test = np.zeros(len(list_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9311a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every alpha train model and save score values\n",
    "count = 0\n",
    "for alpha in list_alpha:\n",
    "    NB = MultinomialNB(alpha=alpha)\n",
    "    NB.fit(X_train_count, y_train)\n",
    "    \n",
    "    score_train[count] = NB.score(X_train_count, y_train)    \n",
    "    score_test[count]= NB.score(X_test_count, y_test)\n",
    "    f1_test[count] = f1_score(y_test, NB.predict(X_test_count))\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9342f7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_F1-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.928142</td>\n",
       "      <td>0.877254</td>\n",
       "      <td>0.814660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.931534</td>\n",
       "      <td>0.877254</td>\n",
       "      <td>0.814271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.927988</td>\n",
       "      <td>0.876560</td>\n",
       "      <td>0.813808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.928296</td>\n",
       "      <td>0.876560</td>\n",
       "      <td>0.813417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.11</td>\n",
       "      <td>0.925906</td>\n",
       "      <td>0.877254</td>\n",
       "      <td>0.812301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alpha  train_accuracy  test_accuracy  test_F1-measure\n",
       "7    0.71        0.928142       0.877254         0.814660\n",
       "2    0.21        0.931534       0.877254         0.814271\n",
       "8    0.81        0.927988       0.876560         0.813808\n",
       "6    0.61        0.928296       0.876560         0.813417\n",
       "11   1.11        0.925906       0.877254         0.812301"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save values to dataframe\n",
    "matrix = np.matrix(np.c_[list_alpha, score_train, score_test, f1_test])\n",
    "models = pd.DataFrame(data=matrix, columns=['alpha', 'train_accuracy', 'test_accuracy', 'test_F1-measure'])\n",
    "#display best test F1-meausure and its alpha\n",
    "display(models.sort_values('test_F1-measure', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb031e",
   "metadata": {},
   "source": [
    "Лучшее значение F1-меры у классификатора при alpha=0.71.\n",
    "\n",
    "Для параметра class_prior установим значение, равное распределению классов во всем датасете -- 2:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02d3f835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.91      0.91       962\n",
      "         1.0       0.82      0.81      0.81       480\n",
      "\n",
      "    accuracy                           0.88      1442\n",
      "   macro avg       0.86      0.86      0.86      1442\n",
      "weighted avg       0.88      0.88      0.88      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Classifier (on count vectors)\n",
    "NB_count = MultinomialNB(alpha=0.71, class_prior=[0.66, 0.33])\n",
    "NB_count.fit(X_train_count, y_train)\n",
    "\n",
    "#metrics\n",
    "NB_count_preds = NB_count.predict(X_test_count)\n",
    "print(classification_report(y_test, NB_count_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1001a1",
   "metadata": {},
   "source": [
    "Теперь на векторах TF-IDF обучим классификатор с помощью логистической регрессии. В качестве вручную заданных параметров будем использовать C (интенсивность регуляризации), penalty (метод регуляризации), solver (алгоритм оптимизации). Для solver и penalty параметры выставим 'liblinear' (подходит для небольших датасетов и бинарной классификации) и 'l2' соответственно. Параметр С попробуем подобрать аналогично подбору alpha для наивного байеса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d14e0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a list of C (from 0.1 to 50 with step=0.1)\n",
    "list_C = np.arange(0.1, 50, 0.1)\n",
    "#set lists for scores\n",
    "score_train = np.zeros(len(list_C))\n",
    "score_test = np.zeros(len(list_C))\n",
    "f1_test = np.zeros(len(list_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9c258cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every C train model and save score values\n",
    "count = 0\n",
    "for C in list_C:\n",
    "    LR = LogisticRegression(C=C, max_iter=1000, penalty='l2', solver='liblinear')\n",
    "    LR.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    score_train[count] = LR.score(X_train_tfidf, y_train)    \n",
    "    score_test[count]= LR.score(X_test_tfidf, y_test)\n",
    "    f1_test[count] = f1_score(y_test, LR.predict(X_test_tfidf))\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "676346b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_F1-measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.970702</td>\n",
       "      <td>0.874480</td>\n",
       "      <td>0.800441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.971473</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>6.9</td>\n",
       "      <td>0.971396</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.971627</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.971935</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7.3</td>\n",
       "      <td>0.972167</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.971010</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.799559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.971010</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.799559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>7.7</td>\n",
       "      <td>0.972938</td>\n",
       "      <td>0.873093</td>\n",
       "      <td>0.799122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.970316</td>\n",
       "      <td>0.873786</td>\n",
       "      <td>0.799117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C  train_accuracy  test_accuracy  test_F1-measure\n",
       "64  6.5        0.970702       0.874480         0.800441\n",
       "69  7.0        0.971473       0.873786         0.800000\n",
       "68  6.9        0.971396       0.873786         0.800000\n",
       "70  7.1        0.971627       0.873786         0.800000\n",
       "71  7.2        0.971935       0.873786         0.800000\n",
       "72  7.3        0.972167       0.873786         0.800000\n",
       "65  6.6        0.971010       0.873786         0.799559\n",
       "66  6.7        0.971010       0.873786         0.799559\n",
       "76  7.7        0.972938       0.873093         0.799122\n",
       "63  6.4        0.970316       0.873786         0.799117"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#save values to dataframe\n",
    "matrix = np.matrix(np.c_[list_C, score_train, score_test, f1_test])\n",
    "models = pd.DataFrame(data=matrix, columns=['C', 'train_accuracy', 'test_accuracy', 'test_F1-measure'])\n",
    "#display best test F1-meausure and its С\n",
    "display(models.sort_values('test_F1-measure', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8e969aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.93      0.91       962\n",
      "         1.0       0.85      0.76      0.80       480\n",
      "\n",
      "    accuracy                           0.87      1442\n",
      "   macro avg       0.87      0.84      0.85      1442\n",
      "weighted avg       0.87      0.87      0.87      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LogisticRegression (on tfidf vectors)\n",
    "LR_tfidf = LogisticRegression(C=6.5, penalty='l2', solver='liblinear')\n",
    "LR_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "#metrics\n",
    "LR_tfidf_preds =LR_tfidf.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, LR_tfidf_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51a2d8",
   "metadata": {},
   "source": [
    "Теперь посмотрим с помощью метода predict_proba на самые токсичные комментарии для каждого классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0f43466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>Стас, никому, кроме тебя и армии твоих подсосов(которые представляют собой типичный дегенеративный биомусор, ведущийся на любые скандалы-интриги), твои ролики нахуй не нужны. Серьёзно, ты сделал новости с целью показать, что такое говно может делать любой, а аудитория осталась на том же уровне, ведь людям извне ты не интересен. Да ещё и просит не подписываться, чтобы такую-то годноту ложкой хлебать подольше. Ты обосрался, стал посмешищем для абсолютно всех ютуберов, которые не являются полными ебланами. Тот же Хованский не ссыт тебе на ебало только потому, что ты вертишься с ним в одной компании, иногда даже лично пересекаетесь. Приятно было слышать, как он говорил, что отстреливался бы от таких, как ты, из огнестрела, стараясь забрать с собой побольше коммунистов, когда они придут его оаскулачиапть? Он открыто хуесосил людей и за меньшие грехи. Сложи 2 и 2, как он к тебе относится на самом деле. После чего ты сделал ещё более смешной ролик, где истеришь как побитая шлюха во время ПМС. Я ПОДЕБИЛ, А ЕСЛИ ВЫ НЕ ПОНЯЛИ ЭТОГО, ТО ВЫ ТУПЫЕ . Ты мог хотя бы сам его посмотреть перед заливом? Мне даже рофлить над тобой расхотелось, из смешного дегенерата, ты стал жалким дурачком. Это как смеятся над роликами, где контуженные ветераны пытаются ходить под клубную музыку. Над неполноценными смеятся плохо, даже стыдно стало. Я не утрирую. Просто посмотри на себя, Стас. Ну правда. Банишь людей в группе за лвйки и одно упоминание стрима. Ты делаешь всё, в чём самый отбитый и дегенеративный либераст обвиняет совок и сверкаешь разорванным очком. Никто тебя несправедливо не обсирал. Что на стриме по поводу дат, ну ты же сам проебался. На подкасте сообщил, что не будешь стримить. Если ты не был уверен, то зачем это говорить? А если был, почему не сообщил Маргиналу сразу же? Твоё слово в целом не стоит нихуя. Обещаешь не банить-куча удалённых комментов. Обещаешь стрим-не идёшь. Обещаешь что-то ещё, всегда проёбываешься, всё чаще на нарушение обещания тебе нужно в районе секунды-дня. Стоит ли удивлятся, что тебе за это прилетело? Когда то должно было. Ты сам срёшь себе в штаны, не злись, когда на это указывают пальцем.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>А сейчас смотрит хуйню всякую с пидорасом звоновым, В оправдание пидораса Звонова можно хотя бы сказать, что последний только и делал контент у чма на стримах: начиная с походов на спасс, новых гостей, научпопвидосов, заканчивая пародией на Что Где Когда. Вообще, после окончания лета я как-то переосмыслил свое отношение ко многим членам инвалидной конфы, да и к самому Маргиналу. Он часто пользуется контентом и талантами своих гостей, приглашая их за нихуя на стримы. При этом хуесосит их же за спиной и на других стримах, из-за чего всё, что гости в итоге получают в ответ за совместные стримы - это говно от маргинальных подписчиков у себя в фиде. И при этом Маргиналу хватает наглости потом ныть, что никто из нормальных людей к нему на стрим не хочет идти, увидав его стенку. Ему не приходит в голову, что поощряя токсичность в отношении своих оппонентов и гостей на стримах он, хотя и обретает ореол интеллектуального стримера, сливателя всяго и вся живого на руси, но по итогу все больше окукливается в собственной же аргументации и аудитории. Заметьте, что те, с кем он еще год назад дебатировал и спорил, сейчас просто избегают споров на стримах, просто не хотя засёра своей ленты (Левин тому ярчайщий пример. Или, уж простите, Звонов). Если раньше Маргоша был способен в самоиронию благодаря тем же убермемес, где замечали и косяки самого Маргинала, то с киком Льва Шойгу начался просто процесс окукливания. Теперь хуесосить Маргинала - запрещено, запрещено даже обсуждать и замечать тупость маргинала. Потому что, как говорит чмо: влияет на просмотры , ололо. По сути, запретив пятнать свою репутацию, но продолжая пятнать репутацию других (и друзей, и врагов) чмо сейчас пытается, сознательно или нет, построить образ эдакого интеллектуала-сливателя, каким бы мечтал стать Ларин. Только вот этот образ Маргинал пока попросту не заслужил. Недостаточно просто говорить ну это просто твое определение пиздец он дурак да, охуенно в ответ на аргументацию. Если раньше, причем, это было редкостью, то сейчас Маргинал совсем обленился и только так и дискутирует под улюлюканье анально модерируемого чатика. Safe space такой safe space.\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>Не зря, вас, хохлов, свиньями кличут. Вы и есть грязные животные, не способные к любви, привязанности, благодарности. Очень радостно видеть, как ваше правительство лижет жопу ЕС, в итоге получая новые порции фекалий, пережёвывает их беззубым ртом, а потом говорит - ещё! У вас самые дешёвые шлюхи в ЕС, да их и столько, что каждому двачеру из Европы достанется.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5536</th>\n",
       "      <td>Стас, никому, кроме тебя и армии твоих подсосов(которые представляют собой типичный дегенеративный биомусор, ведущийся на любые скандалы-интриги), твои ролики нахуй не нужны. Серьёзно, ты сделал новости с целью показать, что такое говно может делать любой, а аудитория осталась на том же уровне, ведь людям извне ты не интересен. Да ещё и просит не подписываться, чтобы такую-то годноту ложкой хлебать подольше. Ты обосрался, стал посмешищем для абсолютно всех ютуберов, которые не являются полными ебланами. Тот же Хованский не ссыт тебе на ебало только потому, что ты вертишься с ним в одной компании, иногда даже лично пересекаетесь. Приятно было слышать, как он говорил, что отстреливался бы от таких, как ты, из огнестрела, стараясь забрать с собой побольше коммунистов, когда они придут его оаскулачиапть? Он открыто хуесосил людей и за меньшие грехи. Сложи 2 и 2, как он к тебе относится на самом деле. После чего ты сделал ещё более смешной ролик, где истеришь как побитая шлюха во время ПМС. Я ПОДЕБИЛ, А ЕСЛИ ВЫ НЕ ПОНЯЛИ ЭТОГО, ТО ВЫ ТУПЫЕ . Ты мог хотя бы сам его посмотреть перед заливом? Мне даже рофлить над тобой расхотелось, из смешного дегенерата, ты стал жалким дурачком. Это как смеятся над роликами, где контуженные ветераны пытаются ходить под клубную музыку. Над неполноценными смеятся плохо, даже стыдно стало. Я не утрирую. Просто посмотри на себя, Стас. Ну правда. Банишь людей в группе за лвйки и одно упоминание стрима. Ты делаешь всё, в чём самый отбитый и дегенеративный либераст обвиняет совок и сверкаешь разорванным очком. Никто тебя несправедливо не обсирал. Что на стриме по поводу дат, ну ты же сам проебался. На подкасте сообщил, что не будешь стримить. Если ты не был уверен, то зачем это говорить? А если был, почему не сообщил Маргиналу сразу же? Твоё слово в целом не стоит нихуя. Обещаешь не банить-куча удалённых комментов. Обещаешь стрим-не идёшь. Обещаешь что-то ещё, всегда проёбываешься, всё чаще на нарушение обещания тебе нужно в районе секунды-дня. Стоит ли удивлятся, что тебе за это прилетело? Когда то должно было. Ты сам срёшь себе в штаны, не злись, когда на это указывают пальцем.\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>Да Евген просто шлюшка без мнения - то блядь пиндосы ему плохие КОКОКО НА КРОВИ ВТОРОЙ МИРОВОЙ ПОДНЯЛИСЬ (намекая на поставки оружия совкам за бабки, только если бы не муриканское вооружение - сосали бы мы все сейчас длинный болт товарища фюрера) То блядь совки ему плохие - сплошная гебня и гулаги, то сука СОВКИ ХОРОШИЕ КОКОКО - НЕПРАВДА НЕ ВСЯ СТРАНА ГУЛАГ. СУКА АЖ ТРЯСЕТ. А споледний обзор - это вообще КРУЖКА - ЛУКЪЯНЕНКО КОКОКО ВЕЛИЧАЙШИЙ ФАНТАСТ СОВРЕМЕННОСТИ Я ЕЩЕ В 2005 НА НЕГО ДРОЧИЛ ПОКА ЭТО НЕ БЫЛО МЭЙНСТРИМОМ КАК ВАМ НЕ СТЫДНО ЗЛОСТНЫЕ КИНОДЕЛЫ ОБСИРАТЬ И ПОГАНИТЬ ТВОРЧЕСТВО ЭТОГО ВЕЛИКОГО ЧЕЛОВЕКА О ЛУКЪЯНЕНКО КОКОКО КОКОКО ДАЙТЕ Я ЕМУ ОТСОСУ и сука тутже через 15 минут АЙ АЙ АЙ АВТОР САМ ОДОБРИЛ ВСЕ ОТКЛОНЕНИЯ ОТ СУЖЕТА КНИГИ КАК НИХОРОШО АЙ АЙ АЙ - но даже тут побоялся сказать прямо - Лукъяненко продался - нет он увиливает и юлит как змея, ак червь, как червь ПИДОР. БЭДКОМЕДИАН - хуже червя ПИДОРА\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>С каких пор порноскримеры нарушают что-то В том и проблемы, что не нарушают ничего, и нашелся умник, начавший этим злоупотреблять и использовать тред для своего увеселения. Свобода ебать, что хочет то и вставляет. Иди поезд-тред создай, зацени свободу. Я уже не говорю об аниме треде, где трут постоянно и хуй пойми что и за что. Там никакой проблемы с удалением постов нет, а как из вебм треда удалить десяток постов, и въебать банхаммером по голове одному (одному, блядь!) дебилу, так это нет, отказывают клавиатуры, мышки, пропадает интернет и отсыхают руки. Ну не могут эти ебаные мочераторы почистить говно там, где их просят. Сделай вебмки тише и будет тебе Щасте. Серьезно? Я должен дрочить звук каждый раз, потому что какой-то дебил решил перед мамкой похвастаться, как он траллит двощи заброской скримеров? Ты гениален, ответ 10 10 просто. Так может и вайпы не трогать, ну не понравился тред кому-то, или просто захотелось повеселиться. Пусть остальные скрывают посты, куклу настраивают.\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>Как известно, у Укр ины (т.е. окр ины), слепленной по пьяни на коленке во 2-м десятилетии XX в., нет истории до XX столетия. Все земли, которые сейчас занимает Укр ина, это русские, румынские, польские и венгерские земли. Напоминаем, что укр инство это сугубо левацкая, антиконсервативная местечково-хуторская идеология, направленная, как и прочие левацкие идеи, на разделение больших наций и поддержание диктата интернацистов. Сторонники бандеровцев (леваков, выступавших за бесклассовое общество и борьбу с капитализмом) и карлика-душителя котов Степана Бандеры, который, как известно, боролся с расизмом, поддерживал Идель-Урал и называл побратимами исламских борцов за свободу из Азербайджана, не пользуются симпатиями у правых европейцев. И это правильно. Попытки заявить о некой отдельной нации неких украинцев это манипуляции, созданные с целью оторвать от русских часть их этнических земель и ослабить в будущем Россию. Только так, чудовищной ложью и тотальной пропагандой, фейковая нация укр инцев , слепленная советскими кукловодами из русских Юга и Киевщины, галичан, поляков, советских румын, славянизированных гуцулов, закарпатских венгров, евреев, татар и многонациональных советских новиопов (а ля Бабченко), может обрести жизнь на русских этнических землях. Разумеется, нет никакого народа укр инцев , как бы одно соседнее failed state ни пыталось их вывести из русских путём обмана, коверканья истории и откровенной фальсификации. Нынешний эксперимент по созданию некой украинской нации можно сравнить разве что с советским экспериментом по созданию нации советской на основании таких же мифов, фейков и откровенного бреда. И маниакальное желание снести все памятники выродку Ленину (Бланку) вас не должно обнадёживать. На смену ему устанавливают памятники такого же левацкого дегенерата-кошкодава Бандеры, чьи руки по локоть в славянской (прежде всего, польской) крови. Заместо совковой лжи про Великую революцию Октября пришла точно такая же наглая ложь про Великую революц ю Г дност абстрагируйтесь от фигуры блогера и посмотрите видео Чем в итоге завершился советский эксперимент, мы все знаем. Ждём закономерного итога эксперимента эльфийского (простите, укр инского ). Разумеется, зомбированные люди будут цепляться до последнего за свои мифы про отельный народ и чужих московитов , но всё это наваждение рано или поздно сгинет, как сгинул Совок со своей мощной идеологией, мифами и фейками.\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13889</th>\n",
       "      <td>Какие блять передергивания? Ты дебил блять зашел на шок-доску и удивляешься что над тобой издеваются. Тут нет твоих друзей, рачье тупорылое, тут тебя все ненавидят. Как же печет от таких необучаемых ебланов. Ты ковбой, твою жену ебут где-то нахуй, а дети гибнут на Украине. Понял, быдло ты ебаное?\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>Предлагаю вашему вниманию поэтические страницы маестра. До тридцати сиянье чуда В тебе, быть может, будет жить Улыбки нежность неоткуда Тебя заставит не забыть Потом морская синь, дороги И быт усталый и глухой Тебя доедут до берлоги И в старость уведут рукой Останутся со мной мгновенья Недокасанья юных рук Недосознанье упоенья Былой любви к тебе, мой друг. 6.03.07. АПОЛОГИЯ ДВИЖЕНИЯ. Среди чернеющих небес Себя найдешь на улице Бредущим по городу Где только снег идет Как ты Другое все иного направленья И сотни тысяч стен домов Тебя встречают В ужасе Ты страшен Поскольку ты идешь Они стоят Тот кто идет всегда опасен И одинок Как этот снег Как эта ночь Огромная удача Что ты не видишь Как ты одинок Но ты уснешь Как это небо станешь Великой темнотой Луна звенит И звезды опадают Как снег, как листопад Куда-то вниз Туда где ты Страниц не различая Запишешь их Листву и звон от звезд И песни стон и лунное дыханье На маятник повяжешь низ Чтоб верх по стрелке шел вперед И только так достигнешь ты себя Своих богов, своих детей Своих последствий И выбор сделаешь на пользу лишь себе 8.03.09. ЧЕЛОВЕК. Такое длинное мгновенье: А дальше эхо и забвенье Что надо нам успеть За этот срок недолгий? Спросить себя: Ты кто? - И дать ответ; Но главное влюбиться: Чтобы ответ мог подтвердиться. 9.01.10. ЮНОСТЬ ПОЭТОВ. Я срезал стебель для своих утех Я гений Значит правом данным от начала Я выбираю жребий тех Кто нужен мне для счастья и причала А, может, для страдания Как агнец богу на закланье На пир души и хищной, и прекрасной Веду я за руку фантомы красоты И каждый ты живешь лишь в мере той Что под руку со мной ведет тебя судьба слепая Ты значим лишь в моих глазах Ты существуешь только как моя болезнь Как озеро испившее нарцисс Как призрак сна настигнувший меня На кромке заколдованного лета Знайте, юность саван воскрешенного поэта 10.02.09.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>Лол, совковая пидораха полыхает, но аргументов кроме ДА ВЫ ЖЫ НИЧИГО НИ ПАНИМАИТИ не принес. Рашка сейчас - это совок, который воюет с другими странами (привет, Афганистан), экономика катится в жопу (привет, дефицит), запрещает иностранные товары (привет, совковые пидорахи, готовые дать в жопу за джинсы), и все это на фоне политического болота (привет, Леонид Ильич)\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                comment  \\\n",
       "1823                                                                                                                                                                                                                                                                                           Стас, никому, кроме тебя и армии твоих подсосов(которые представляют собой типичный дегенеративный биомусор, ведущийся на любые скандалы-интриги), твои ролики нахуй не нужны. Серьёзно, ты сделал новости с целью показать, что такое говно может делать любой, а аудитория осталась на том же уровне, ведь людям извне ты не интересен. Да ещё и просит не подписываться, чтобы такую-то годноту ложкой хлебать подольше. Ты обосрался, стал посмешищем для абсолютно всех ютуберов, которые не являются полными ебланами. Тот же Хованский не ссыт тебе на ебало только потому, что ты вертишься с ним в одной компании, иногда даже лично пересекаетесь. Приятно было слышать, как он говорил, что отстреливался бы от таких, как ты, из огнестрела, стараясь забрать с собой побольше коммунистов, когда они придут его оаскулачиапть? Он открыто хуесосил людей и за меньшие грехи. Сложи 2 и 2, как он к тебе относится на самом деле. После чего ты сделал ещё более смешной ролик, где истеришь как побитая шлюха во время ПМС. Я ПОДЕБИЛ, А ЕСЛИ ВЫ НЕ ПОНЯЛИ ЭТОГО, ТО ВЫ ТУПЫЕ . Ты мог хотя бы сам его посмотреть перед заливом? Мне даже рофлить над тобой расхотелось, из смешного дегенерата, ты стал жалким дурачком. Это как смеятся над роликами, где контуженные ветераны пытаются ходить под клубную музыку. Над неполноценными смеятся плохо, даже стыдно стало. Я не утрирую. Просто посмотри на себя, Стас. Ну правда. Банишь людей в группе за лвйки и одно упоминание стрима. Ты делаешь всё, в чём самый отбитый и дегенеративный либераст обвиняет совок и сверкаешь разорванным очком. Никто тебя несправедливо не обсирал. Что на стриме по поводу дат, ну ты же сам проебался. На подкасте сообщил, что не будешь стримить. Если ты не был уверен, то зачем это говорить? А если был, почему не сообщил Маргиналу сразу же? Твоё слово в целом не стоит нихуя. Обещаешь не банить-куча удалённых комментов. Обещаешь стрим-не идёшь. Обещаешь что-то ещё, всегда проёбываешься, всё чаще на нарушение обещания тебе нужно в районе секунды-дня. Стоит ли удивлятся, что тебе за это прилетело? Когда то должно было. Ты сам срёшь себе в штаны, не злись, когда на это указывают пальцем.   \n",
       "5913                                                                                                                                                                                                                                                                                   А сейчас смотрит хуйню всякую с пидорасом звоновым, В оправдание пидораса Звонова можно хотя бы сказать, что последний только и делал контент у чма на стримах: начиная с походов на спасс, новых гостей, научпопвидосов, заканчивая пародией на Что Где Когда. Вообще, после окончания лета я как-то переосмыслил свое отношение ко многим членам инвалидной конфы, да и к самому Маргиналу. Он часто пользуется контентом и талантами своих гостей, приглашая их за нихуя на стримы. При этом хуесосит их же за спиной и на других стримах, из-за чего всё, что гости в итоге получают в ответ за совместные стримы - это говно от маргинальных подписчиков у себя в фиде. И при этом Маргиналу хватает наглости потом ныть, что никто из нормальных людей к нему на стрим не хочет идти, увидав его стенку. Ему не приходит в голову, что поощряя токсичность в отношении своих оппонентов и гостей на стримах он, хотя и обретает ореол интеллектуального стримера, сливателя всяго и вся живого на руси, но по итогу все больше окукливается в собственной же аргументации и аудитории. Заметьте, что те, с кем он еще год назад дебатировал и спорил, сейчас просто избегают споров на стримах, просто не хотя засёра своей ленты (Левин тому ярчайщий пример. Или, уж простите, Звонов). Если раньше Маргоша был способен в самоиронию благодаря тем же убермемес, где замечали и косяки самого Маргинала, то с киком Льва Шойгу начался просто процесс окукливания. Теперь хуесосить Маргинала - запрещено, запрещено даже обсуждать и замечать тупость маргинала. Потому что, как говорит чмо: влияет на просмотры , ололо. По сути, запретив пятнать свою репутацию, но продолжая пятнать репутацию других (и друзей, и врагов) чмо сейчас пытается, сознательно или нет, построить образ эдакого интеллектуала-сливателя, каким бы мечтал стать Ларин. Только вот этот образ Маргинал пока попросту не заслужил. Недостаточно просто говорить ну это просто твое определение пиздец он дурак да, охуенно в ответ на аргументацию. Если раньше, причем, это было редкостью, то сейчас Маргинал совсем обленился и только так и дискутирует под улюлюканье анально модерируемого чатика. Safe space такой safe space.\\n   \n",
       "1913                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Не зря, вас, хохлов, свиньями кличут. Вы и есть грязные животные, не способные к любви, привязанности, благодарности. Очень радостно видеть, как ваше правительство лижет жопу ЕС, в итоге получая новые порции фекалий, пережёвывает их беззубым ртом, а потом говорит - ещё! У вас самые дешёвые шлюхи в ЕС, да их и столько, что каждому двачеру из Европы достанется.   \n",
       "5536                                                                                                                                                                                                                                                                                         Стас, никому, кроме тебя и армии твоих подсосов(которые представляют собой типичный дегенеративный биомусор, ведущийся на любые скандалы-интриги), твои ролики нахуй не нужны. Серьёзно, ты сделал новости с целью показать, что такое говно может делать любой, а аудитория осталась на том же уровне, ведь людям извне ты не интересен. Да ещё и просит не подписываться, чтобы такую-то годноту ложкой хлебать подольше. Ты обосрался, стал посмешищем для абсолютно всех ютуберов, которые не являются полными ебланами. Тот же Хованский не ссыт тебе на ебало только потому, что ты вертишься с ним в одной компании, иногда даже лично пересекаетесь. Приятно было слышать, как он говорил, что отстреливался бы от таких, как ты, из огнестрела, стараясь забрать с собой побольше коммунистов, когда они придут его оаскулачиапть? Он открыто хуесосил людей и за меньшие грехи. Сложи 2 и 2, как он к тебе относится на самом деле. После чего ты сделал ещё более смешной ролик, где истеришь как побитая шлюха во время ПМС. Я ПОДЕБИЛ, А ЕСЛИ ВЫ НЕ ПОНЯЛИ ЭТОГО, ТО ВЫ ТУПЫЕ . Ты мог хотя бы сам его посмотреть перед заливом? Мне даже рофлить над тобой расхотелось, из смешного дегенерата, ты стал жалким дурачком. Это как смеятся над роликами, где контуженные ветераны пытаются ходить под клубную музыку. Над неполноценными смеятся плохо, даже стыдно стало. Я не утрирую. Просто посмотри на себя, Стас. Ну правда. Банишь людей в группе за лвйки и одно упоминание стрима. Ты делаешь всё, в чём самый отбитый и дегенеративный либераст обвиняет совок и сверкаешь разорванным очком. Никто тебя несправедливо не обсирал. Что на стриме по поводу дат, ну ты же сам проебался. На подкасте сообщил, что не будешь стримить. Если ты не был уверен, то зачем это говорить? А если был, почему не сообщил Маргиналу сразу же? Твоё слово в целом не стоит нихуя. Обещаешь не банить-куча удалённых комментов. Обещаешь стрим-не идёшь. Обещаешь что-то ещё, всегда проёбываешься, всё чаще на нарушение обещания тебе нужно в районе секунды-дня. Стоит ли удивлятся, что тебе за это прилетело? Когда то должно было. Ты сам срёшь себе в штаны, не злись, когда на это указывают пальцем.\\n   \n",
       "2354                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Да Евген просто шлюшка без мнения - то блядь пиндосы ему плохие КОКОКО НА КРОВИ ВТОРОЙ МИРОВОЙ ПОДНЯЛИСЬ (намекая на поставки оружия совкам за бабки, только если бы не муриканское вооружение - сосали бы мы все сейчас длинный болт товарища фюрера) То блядь совки ему плохие - сплошная гебня и гулаги, то сука СОВКИ ХОРОШИЕ КОКОКО - НЕПРАВДА НЕ ВСЯ СТРАНА ГУЛАГ. СУКА АЖ ТРЯСЕТ. А споледний обзор - это вообще КРУЖКА - ЛУКЪЯНЕНКО КОКОКО ВЕЛИЧАЙШИЙ ФАНТАСТ СОВРЕМЕННОСТИ Я ЕЩЕ В 2005 НА НЕГО ДРОЧИЛ ПОКА ЭТО НЕ БЫЛО МЭЙНСТРИМОМ КАК ВАМ НЕ СТЫДНО ЗЛОСТНЫЕ КИНОДЕЛЫ ОБСИРАТЬ И ПОГАНИТЬ ТВОРЧЕСТВО ЭТОГО ВЕЛИКОГО ЧЕЛОВЕКА О ЛУКЪЯНЕНКО КОКОКО КОКОКО ДАЙТЕ Я ЕМУ ОТСОСУ и сука тутже через 15 минут АЙ АЙ АЙ АВТОР САМ ОДОБРИЛ ВСЕ ОТКЛОНЕНИЯ ОТ СУЖЕТА КНИГИ КАК НИХОРОШО АЙ АЙ АЙ - но даже тут побоялся сказать прямо - Лукъяненко продался - нет он увиливает и юлит как змея, ак червь, как червь ПИДОР. БЭДКОМЕДИАН - хуже червя ПИДОРА\\n   \n",
       "264                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              С каких пор порноскримеры нарушают что-то В том и проблемы, что не нарушают ничего, и нашелся умник, начавший этим злоупотреблять и использовать тред для своего увеселения. Свобода ебать, что хочет то и вставляет. Иди поезд-тред создай, зацени свободу. Я уже не говорю об аниме треде, где трут постоянно и хуй пойми что и за что. Там никакой проблемы с удалением постов нет, а как из вебм треда удалить десяток постов, и въебать банхаммером по голове одному (одному, блядь!) дебилу, так это нет, отказывают клавиатуры, мышки, пропадает интернет и отсыхают руки. Ну не могут эти ебаные мочераторы почистить говно там, где их просят. Сделай вебмки тише и будет тебе Щасте. Серьезно? Я должен дрочить звук каждый раз, потому что какой-то дебил решил перед мамкой похвастаться, как он траллит двощи заброской скримеров? Ты гениален, ответ 10 10 просто. Так может и вайпы не трогать, ну не понравился тред кому-то, или просто захотелось повеселиться. Пусть остальные скрывают посты, куклу настраивают.\\n   \n",
       "723    Как известно, у Укр ины (т.е. окр ины), слепленной по пьяни на коленке во 2-м десятилетии XX в., нет истории до XX столетия. Все земли, которые сейчас занимает Укр ина, это русские, румынские, польские и венгерские земли. Напоминаем, что укр инство это сугубо левацкая, антиконсервативная местечково-хуторская идеология, направленная, как и прочие левацкие идеи, на разделение больших наций и поддержание диктата интернацистов. Сторонники бандеровцев (леваков, выступавших за бесклассовое общество и борьбу с капитализмом) и карлика-душителя котов Степана Бандеры, который, как известно, боролся с расизмом, поддерживал Идель-Урал и называл побратимами исламских борцов за свободу из Азербайджана, не пользуются симпатиями у правых европейцев. И это правильно. Попытки заявить о некой отдельной нации неких украинцев это манипуляции, созданные с целью оторвать от русских часть их этнических земель и ослабить в будущем Россию. Только так, чудовищной ложью и тотальной пропагандой, фейковая нация укр инцев , слепленная советскими кукловодами из русских Юга и Киевщины, галичан, поляков, советских румын, славянизированных гуцулов, закарпатских венгров, евреев, татар и многонациональных советских новиопов (а ля Бабченко), может обрести жизнь на русских этнических землях. Разумеется, нет никакого народа укр инцев , как бы одно соседнее failed state ни пыталось их вывести из русских путём обмана, коверканья истории и откровенной фальсификации. Нынешний эксперимент по созданию некой украинской нации можно сравнить разве что с советским экспериментом по созданию нации советской на основании таких же мифов, фейков и откровенного бреда. И маниакальное желание снести все памятники выродку Ленину (Бланку) вас не должно обнадёживать. На смену ему устанавливают памятники такого же левацкого дегенерата-кошкодава Бандеры, чьи руки по локоть в славянской (прежде всего, польской) крови. Заместо совковой лжи про Великую революцию Октября пришла точно такая же наглая ложь про Великую революц ю Г дност абстрагируйтесь от фигуры блогера и посмотрите видео Чем в итоге завершился советский эксперимент, мы все знаем. Ждём закономерного итога эксперимента эльфийского (простите, укр инского ). Разумеется, зомбированные люди будут цепляться до последнего за свои мифы про отельный народ и чужих московитов , но всё это наваждение рано или поздно сгинет, как сгинул Совок со своей мощной идеологией, мифами и фейками.\\n   \n",
       "13889                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Какие блять передергивания? Ты дебил блять зашел на шок-доску и удивляешься что над тобой издеваются. Тут нет твоих друзей, рачье тупорылое, тут тебя все ненавидят. Как же печет от таких необучаемых ебланов. Ты ковбой, твою жену ебут где-то нахуй, а дети гибнут на Украине. Понял, быдло ты ебаное?\\n   \n",
       "1877                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Предлагаю вашему вниманию поэтические страницы маестра. До тридцати сиянье чуда В тебе, быть может, будет жить Улыбки нежность неоткуда Тебя заставит не забыть Потом морская синь, дороги И быт усталый и глухой Тебя доедут до берлоги И в старость уведут рукой Останутся со мной мгновенья Недокасанья юных рук Недосознанье упоенья Былой любви к тебе, мой друг. 6.03.07. АПОЛОГИЯ ДВИЖЕНИЯ. Среди чернеющих небес Себя найдешь на улице Бредущим по городу Где только снег идет Как ты Другое все иного направленья И сотни тысяч стен домов Тебя встречают В ужасе Ты страшен Поскольку ты идешь Они стоят Тот кто идет всегда опасен И одинок Как этот снег Как эта ночь Огромная удача Что ты не видишь Как ты одинок Но ты уснешь Как это небо станешь Великой темнотой Луна звенит И звезды опадают Как снег, как листопад Куда-то вниз Туда где ты Страниц не различая Запишешь их Листву и звон от звезд И песни стон и лунное дыханье На маятник повяжешь низ Чтоб верх по стрелке шел вперед И только так достигнешь ты себя Своих богов, своих детей Своих последствий И выбор сделаешь на пользу лишь себе 8.03.09. ЧЕЛОВЕК. Такое длинное мгновенье: А дальше эхо и забвенье Что надо нам успеть За этот срок недолгий? Спросить себя: Ты кто? - И дать ответ; Но главное влюбиться: Чтобы ответ мог подтвердиться. 9.01.10. ЮНОСТЬ ПОЭТОВ. Я срезал стебель для своих утех Я гений Значит правом данным от начала Я выбираю жребий тех Кто нужен мне для счастья и причала А, может, для страдания Как агнец богу на закланье На пир души и хищной, и прекрасной Веду я за руку фантомы красоты И каждый ты живешь лишь в мере той Что под руку со мной ведет тебя судьба слепая Ты значим лишь в моих глазах Ты существуешь только как моя болезнь Как озеро испившее нарцисс Как призрак сна настигнувший меня На кромке заколдованного лета Знайте, юность саван воскрешенного поэта 10.02.09.   \n",
       "666                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Лол, совковая пидораха полыхает, но аргументов кроме ДА ВЫ ЖЫ НИЧИГО НИ ПАНИМАИТИ не принес. Рашка сейчас - это совок, который воюет с другими странами (привет, Афганистан), экономика катится в жопу (привет, дефицит), запрещает иностранные товары (привет, совковые пидорахи, готовые дать в жопу за джинсы), и все это на фоне политического болота (привет, Леонид Ильич)\\n   \n",
       "\n",
       "       toxic_probability  \n",
       "1823                 1.0  \n",
       "5913                 1.0  \n",
       "1913                 1.0  \n",
       "5536                 1.0  \n",
       "2354                 1.0  \n",
       "264                  1.0  \n",
       "723                  1.0  \n",
       "13889                1.0  \n",
       "1877                 1.0  \n",
       "666                  1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#probas for NB\n",
    "NB_probas = pd.DataFrame(test.comment)\n",
    "NB_probas['toxic_probability'] = NB_count.predict_proba(X_test_count)[:,1]\n",
    "\n",
    "display(NB_probas.sort_values(['toxic_probability'], ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "809a42b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>Какие же хохлы незалежные дегенераты, пиздец просто.\\n</td>\n",
       "      <td>0.999866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6314</th>\n",
       "      <td>Плюсы: -Какие же хохлы дегенераты, пиздец просто. Минусы: -Какие же хохлв дегенераты, пиздец просто.\\n</td>\n",
       "      <td>0.999832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6497</th>\n",
       "      <td>null 0 Сука, какие же коммибляди тупые.\\n</td>\n",
       "      <td>0.999716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13642</th>\n",
       "      <td>мандан против хохлов?\\n</td>\n",
       "      <td>0.999679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>Когда тред прощания с хохлами будет?\\n</td>\n",
       "      <td>0.999664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>Потому что сенегалец лучше хохла. Хохлы вообще не люди\\n</td>\n",
       "      <td>0.999638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13889</th>\n",
       "      <td>Какие блять передергивания? Ты дебил блять зашел на шок-доску и удивляешься что над тобой издеваются. Тут нет твоих друзей, рачье тупорылое, тут тебя все ненавидят. Как же печет от таких необучаемых ебланов. Ты ковбой, твою жену ебут где-то нахуй, а дети гибнут на Украине. Понял, быдло ты ебаное?\\n</td>\n",
       "      <td>0.999492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>Нахуй иди, я тебе весь тред что ли читать буду? Пидор, бешбармак тебе в хычин!\\n</td>\n",
       "      <td>0.999453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5720</th>\n",
       "      <td>Свин, иди нахуй. Можно хоть один тред без политоты? Даже тебя хуеосить не хочется, давай лучше про няшек-фигуристок говорить.\\n</td>\n",
       "      <td>0.999221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>НУ ВСЕ, сука говго щас модераторам пишу твою тему удалят нахуй к хуям говняным\\n</td>\n",
       "      <td>0.999064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                           comment  \\\n",
       "2061                                                                                                                                                                                                                                                        Какие же хохлы незалежные дегенераты, пиздец просто.\\n   \n",
       "6314                                                                                                                                                                                                        Плюсы: -Какие же хохлы дегенераты, пиздец просто. Минусы: -Какие же хохлв дегенераты, пиздец просто.\\n   \n",
       "6497                                                                                                                                                                                                                                                                     null 0 Сука, какие же коммибляди тупые.\\n   \n",
       "13642                                                                                                                                                                                                                                                                                      мандан против хохлов?\\n   \n",
       "2019                                                                                                                                                                                                                                                                        Когда тред прощания с хохлами будет?\\n   \n",
       "6432                                                                                                                                                                                                                                                      Потому что сенегалец лучше хохла. Хохлы вообще не люди\\n   \n",
       "13889  Какие блять передергивания? Ты дебил блять зашел на шок-доску и удивляешься что над тобой издеваются. Тут нет твоих друзей, рачье тупорылое, тут тебя все ненавидят. Как же печет от таких необучаемых ебланов. Ты ковбой, твою жену ебут где-то нахуй, а дети гибнут на Украине. Понял, быдло ты ебаное?\\n   \n",
       "2011                                                                                                                                                                                                                              Нахуй иди, я тебе весь тред что ли читать буду? Пидор, бешбармак тебе в хычин!\\n   \n",
       "5720                                                                                                                                                                               Свин, иди нахуй. Можно хоть один тред без политоты? Даже тебя хуеосить не хочется, давай лучше про няшек-фигуристок говорить.\\n   \n",
       "2080                                                                                                                                                                                                                              НУ ВСЕ, сука говго щас модераторам пишу твою тему удалят нахуй к хуям говняным\\n   \n",
       "\n",
       "       toxic_probability  \n",
       "2061            0.999866  \n",
       "6314            0.999832  \n",
       "6497            0.999716  \n",
       "13642           0.999679  \n",
       "2019            0.999664  \n",
       "6432            0.999638  \n",
       "13889           0.999492  \n",
       "2011            0.999453  \n",
       "5720            0.999221  \n",
       "2080            0.999064  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#probas for LR\n",
    "LR_probas = pd.DataFrame(test.comment)\n",
    "LR_probas['toxic_probability'] = LR_tfidf.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "display(LR_probas.sort_values(['toxic_probability'], ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c058530",
   "metadata": {},
   "source": [
    "Для логистической регрессии все 10 топ-токсичных комментариев действительно токсичные, для наивного байеса есть один, который кажется странным, но при этом не является оскорбительным. Заметное отличие -- длина комментариев (наивный байес к самым токсичным комментариям относит довольно длинные документы, в отличие от логистической регрессии)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68324753",
   "metadata": {},
   "source": [
    "## *Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7794f97",
   "metadata": {},
   "source": [
    "Для классификаторов LogisticRegression, Decision Trees, Naive Bayes, Random Forest найдите способ извлечь важность признаков для предсказания токсичного класса. Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов. \n",
    "\n",
    "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e7266",
   "metadata": {},
   "source": [
    "**Векторизация**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e4c147",
   "metadata": {},
   "source": [
    "Для начала разбиваем датасет на обучающую и тестовую выборки (данные будут одинаковые для всех классификаторов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65e99fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7228b21",
   "metadata": {},
   "source": [
    "Для того, чтобы избавиться от стоп-слов, в векторайзер можно подать соответствующий список. Готовый список стоп-слов есть, например, в nltk, но он неполный, поэтому нам необходимо его расширить за счет слов, которые в нашем датасете встречаются одинаково часто в токсичных и нетоксичных комментариях. Для этого сформируем для каждого класса комментариев список наиболее часто встречающихся слов (возьмем по 300 для каждого класса, ранжировать будем по относительной частоте) и добавим к списку из nltk те из них, которые встречаются в обоих списках. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d5b23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk list of stop-words\n",
    "stops = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fcfcac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for counting frequency of words in corpus\n",
    "def words_counter(corpus):\n",
    "    all_tokens = []\n",
    "    for document in corpus:\n",
    "        for token in re.findall(r'\\b\\w+-?\\w+\\b', document.lower()):\n",
    "            all_tokens.append(token)\n",
    "    words = Counter(all_tokens)\n",
    "    for word in words:\n",
    "        words[word] /= len(words)  \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cacc8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    }
   ],
   "source": [
    "#adding frequent words to stop-words list\n",
    "toxic = [word[0] for word in words_counter(data.comment[data.toxic==1]).most_common(300)]\n",
    "not_toxic = [word[0] for word in words_counter(data.comment[data.toxic==0]).most_common(300)]\n",
    "\n",
    "count = 0\n",
    "for word in list(set(toxic) & set(not_toxic)):\n",
    "    if word not in stops:\n",
    "        stops.append(word)\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6f65f7",
   "metadata": {},
   "source": [
    "Список стоп-слов увеличился на 88 токенов. Этот список подадим как параметр stop_words для векторайзера. Чтобы уменьшить количество признаков, установим параметр min_df (при изменении max_df количество признаков почти не меняется -- это связано с тем, что мы уже используем очистку от стоп-слов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e94e3b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12970, 13827) (1442, 13827)\n"
     ]
    }
   ],
   "source": [
    "#build feature vectors\n",
    "vectorizer = CountVectorizer(stop_words=stops, min_df=3)\n",
    "X_train = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment) \n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5f5868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12970,) (1442,)\n"
     ]
    }
   ],
   "source": [
    "#target values\n",
    "y_train = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253e97a1",
   "metadata": {},
   "source": [
    "**Важность признаков для Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27f71e1",
   "metadata": {},
   "source": [
    "У логистической регрессии важность признаков показывают коэффициенты признаков в функции. Они вызываются свойством coef_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaf9062a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learning\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "879dfaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>LR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>хохлов</td>\n",
       "      <td>3.117413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13147</th>\n",
       "      <td>хохлы</td>\n",
       "      <td>2.851370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>дебил</td>\n",
       "      <td>2.796493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13150</th>\n",
       "      <td>хохол</td>\n",
       "      <td>2.184779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11769</th>\n",
       "      <td>сука</td>\n",
       "      <td>2.176660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature        LR\n",
       "13143  хохлов  3.117413\n",
       "13147   хохлы  2.851370\n",
       "2347    дебил  2.796493\n",
       "13150   хохол  2.184779\n",
       "11769    сука  2.176660"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importances for features\n",
    "importances = pd.DataFrame(vectorizer.get_feature_names_out(), columns=['feature'])\n",
    "importances['LR'] = LR.coef_[0]\n",
    "\n",
    "display(importances.sort_values(['LR'], ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd1424",
   "metadata": {},
   "source": [
    "**Важность признаков для Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b436b97f",
   "metadata": {},
   "source": [
    "Для просмотра важности признаков у деревьев решений есть свойство feature_importances_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb201073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learning\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56abbb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>LR</th>\n",
       "      <th>DT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13147</th>\n",
       "      <td>хохлы</td>\n",
       "      <td>2.851370</td>\n",
       "      <td>0.015308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>нахуй</td>\n",
       "      <td>1.514811</td>\n",
       "      <td>0.012597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>хохлов</td>\n",
       "      <td>3.117413</td>\n",
       "      <td>0.012341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>блядь</td>\n",
       "      <td>1.638883</td>\n",
       "      <td>0.008233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>блять</td>\n",
       "      <td>1.940730</td>\n",
       "      <td>0.007834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature        LR        DT\n",
       "13147   хохлы  2.851370  0.015308\n",
       "6092    нахуй  1.514811  0.012597\n",
       "13143  хохлов  3.117413  0.012341\n",
       "817     блядь  1.638883  0.008233\n",
       "818     блять  1.940730  0.007834"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importances for features\n",
    "importances['DT'] = DT.feature_importances_\n",
    "display(importances.sort_values(['DT'], ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05632fb",
   "metadata": {},
   "source": [
    "**Важность признаков для Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2432163c",
   "metadata": {},
   "source": [
    "У наивного байеса важность признаков можно извлечь из feature_log_prob_ (свойство возвращает numpy.ndarray, нам необходим второй, для токсичного класса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "249aaa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = MultinomialNB()\n",
    "NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1453100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>LR</th>\n",
       "      <th>DT</th>\n",
       "      <th>NB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>нахуй</td>\n",
       "      <td>1.514811</td>\n",
       "      <td>0.012597</td>\n",
       "      <td>-6.053295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13147</th>\n",
       "      <td>хохлы</td>\n",
       "      <td>2.851370</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>-6.133338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>блядь</td>\n",
       "      <td>1.638883</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>-6.220349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>блять</td>\n",
       "      <td>1.940730</td>\n",
       "      <td>0.007834</td>\n",
       "      <td>-6.251121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>хохлов</td>\n",
       "      <td>3.117413</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>-6.338132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature        LR        DT        NB\n",
       "6092    нахуй  1.514811  0.012597 -6.053295\n",
       "13147   хохлы  2.851370  0.015308 -6.133338\n",
       "817     блядь  1.638883  0.008233 -6.220349\n",
       "818     блять  1.940730  0.007834 -6.251121\n",
       "13143  хохлов  3.117413  0.012341 -6.338132"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances['NB'] = NB.feature_log_prob_[1]\n",
    "display(importances.sort_values(['NB'], ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2862551",
   "metadata": {},
   "source": [
    "**Важность признаков для Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e1f57",
   "metadata": {},
   "source": [
    "Для просмотра важности признаков у деревьев решений есть свойство feature_importances_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4844783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da7cd416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>LR</th>\n",
       "      <th>DT</th>\n",
       "      <th>NB</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13147</th>\n",
       "      <td>хохлы</td>\n",
       "      <td>2.851370</td>\n",
       "      <td>0.015308</td>\n",
       "      <td>-6.133338</td>\n",
       "      <td>0.010295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>хохлов</td>\n",
       "      <td>3.117413</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>-6.338132</td>\n",
       "      <td>0.009757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6092</th>\n",
       "      <td>нахуй</td>\n",
       "      <td>1.514811</td>\n",
       "      <td>0.012597</td>\n",
       "      <td>-6.053295</td>\n",
       "      <td>0.007266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>блядь</td>\n",
       "      <td>1.638883</td>\n",
       "      <td>0.008233</td>\n",
       "      <td>-6.220349</td>\n",
       "      <td>0.005644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11769</th>\n",
       "      <td>сука</td>\n",
       "      <td>2.176660</td>\n",
       "      <td>0.006317</td>\n",
       "      <td>-6.688335</td>\n",
       "      <td>0.004958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature        LR        DT        NB        RF\n",
       "13147   хохлы  2.851370  0.015308 -6.133338  0.010295\n",
       "13143  хохлов  3.117413  0.012341 -6.338132  0.009757\n",
       "6092    нахуй  1.514811  0.012597 -6.053295  0.007266\n",
       "817     блядь  1.638883  0.008233 -6.220349  0.005644\n",
       "11769    сука  2.176660  0.006317 -6.688335  0.004958"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances['RF'] = RF.feature_importances_\n",
    "display(importances.sort_values(['RF'], ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc4fb6",
   "metadata": {},
   "source": [
    "В целом, списки топ-5 токсичных слов для каждого классификатора примерно похожи, все из них действительно обладают высокой степенью токсичности. У деревьев решений,  наивного байеса и случайного леса списки очень похожи (у деревьев решений и наивного байеса списки идентичные)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
