{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44506c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d67cf2",
   "metadata": {},
   "source": [
    "Возьмем для обучения языковой модели корпус текстов Л.Н.Толстого (\"Война и мир\" и \"Анна Каренина\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('tolstoy.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9b092e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем корпуса в символах: 4600956\n"
     ]
    }
   ],
   "source": [
    "print('Объем корпуса в символах:', len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68019ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем корпуса в предложениях: 50531\n"
     ]
    }
   ],
   "source": [
    "sents_tokenized = sent_tokenize(corpus)\n",
    "print('Объем корпуса в предложениях:', len(sents_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb420d",
   "metadata": {},
   "source": [
    "Далее нормализуем предложения (удаляем пунктуацию, приводим к нижнему регистру) и добавляем к токенизированным предложениям два тега в начало \\<start> \\<start> (чтобы строить вероятность по двум предыдущим токенам) и один тег \\<end> в конец. Далее выделяем часть корпуса на train, чтобы посчитать перплексию (выборку для train возьмем небольшую, около 0.05%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d639bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text_normalized = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\n",
    "    text_normalized = [word.lower() for word in text_normalized if word]\n",
    "    return text_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3322c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_normalized = []\n",
    "for sent in sents_tokenized:\n",
    "    sent = normalize(sent)\n",
    "    sent.insert(0, '<start>')\n",
    "    sent.insert(0, '<start>')\n",
    "    sent.append('<end>')\n",
    "    sents_normalized.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200d46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Объем тестовой выборки: 26\n"
     ]
    }
   ],
   "source": [
    "X, y = train_test_split(sents_normalized, test_size = 0.0005, shuffle=True)\n",
    "print('Объем тестовой выборки:', len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8cd31",
   "metadata": {},
   "source": [
    "Составляем частнотности для 1-грамм, 2-грамм и 3-грамм для подсчета статистик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "605c002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "233c7a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "for sent in X:\n",
    "    unigrams.update(sent)\n",
    "    bigrams.update(ngrammer(sent, 2))\n",
    "    trigrams.update(ngrammer(sent, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9b1d5",
   "metadata": {},
   "source": [
    "Посмотрим на наиболее часто встречающиеся биграммы и триграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd99146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> <start>', 50505)\n",
      "('<start> —', 13968)\n",
      "('<start> он', 2357)\n",
      "('— сказал', 2324)\n",
      "('<start> и', 1647)\n",
      "('что он', 1484)\n",
      "('<start> но', 1339)\n",
      "('<start> она', 1291)\n",
      "('и не', 1231)\n",
      "('— сказала', 1118)\n"
     ]
    }
   ],
   "source": [
    "for i in bigrams.most_common(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb34aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> <start> —', 13968)\n",
      "('<start> <start> он', 2357)\n",
      "('<start> <start> и', 1647)\n",
      "('<start> <start> но', 1339)\n",
      "('<start> <start> она', 1291)\n",
      "('<start> <start> в', 1084)\n",
      "('<start> <start> я', 1048)\n",
      "('<start> — я', 883)\n",
      "('— сказал он', 847)\n",
      "('<start> — да', 761)\n"
     ]
    }
   ],
   "source": [
    "for i in trigrams.most_common(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd4b28f",
   "metadata": {},
   "source": [
    "Составляем матрицу вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e72a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_corpus = lil_matrix((len(bigrams), len(unigrams)))\n",
    "\n",
    "id2unigram = list(unigrams)\n",
    "unigram2id = {word: i for i, word in enumerate(id2unigram)}\n",
    "\n",
    "id2bigram = list(bigrams)\n",
    "bigram2id = {word: i for i, word in enumerate(id2bigram)}\n",
    "\n",
    "for ngram in trigrams:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = word1 + ' ' + word2\n",
    "    matrix_corpus[bigram2id[bigram], unigram2id[word3]] = (trigrams[ngram]/bigrams[bigram])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b835c",
   "metadata": {},
   "source": [
    "И теперь генерируем тексты с помощью модели: на вход изначально подаются токены \\<start> \\<start>, далее рандомно, но с учетом вероятностей, выбирается униграмма. Далее на вход подается токен \\<start> и выбранный на предыдущем шаге токен, и снова выбирается следующий вероятный токен и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6972487a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id_to_unigram, bigram_to_id, n=100, start='<start> <start>'):\n",
    "    \n",
    "    text = [[]]\n",
    "    current_idx = bigram_to_id[start]\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(list(range(matrix.shape[1])), p=matrix[current_idx].toarray()[0])\n",
    "        text[count].append(id_to_unigram[chosen])\n",
    "        \n",
    "        if len(text[count]) == 1:\n",
    "            current_idx = bigram_to_id['<start>' + ' ' + text[count][0]]\n",
    "        \n",
    "        if len(text[count]) > 1:\n",
    "            current_idx = bigram_to_id[text[count][len(text[count])-2] + ' ' + text[count][len(text[count])-1]]\n",
    "            \n",
    "        if id_to_unigram[chosen] == '<end>':\n",
    "            current_idx = bigram_to_id[start]\n",
    "            count += 1\n",
    "            text.append([]) \n",
    "            \n",
    "    text = ' '.join([' '.join(sent) for sent in text]).replace('<end>', '\\n')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a36018c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "она взглянула на него \n",
      " — ну так так \n",
      " — что я рискую злоупотребить вашим вниманием \n",
      " но дамы невольно смеялись и князь андрей опять к ним французы то староста сказал что он делал и не могу наказать его ежели он сделает дурно и не успел еще бельяр скрыться из вида и вдруг отчаянно бросил ее и перешел на злоупотребления властей в соединенных штатах но анна не отвечая ничего ни слабости ничего я чувствую его \n",
      " — у николеньки есть эта слабость что если бы наполеон успел уйти \n",
      " — а ты \n",
      " она видела \n",
      " передовые ее низкие\n",
      "\n",
      "\n",
      "пустить их и смягчить их возражения \n",
      " в передней увидав пьера он сердито пробормотал что-то и левин был влюблен но — сказал болконский \n",
      " — нет алексей александрович расспросил в чем состоит это благо \n",
      " все выскажу ему все равно не запомню только что вышли \n",
      " одно успокоительное рассуждение о том что облонский улыбался говоря это в такую минуту повторил князь андрей с презрением смотрел на смеющегося сперанского \n",
      " человек этот сводится со всеми вместе \n",
      " — думал князь андрей поехал с покупщиком ладилось и ничто не тревожит его и ни один человек и бывает здесь \n",
      " ну а ты\n",
      "\n",
      "\n",
      "— подтвердили голенищев и в ту минуту на коленях испуганно но прикованно она не хочет а чтоб к нему \n",
      " — еще бы \n",
      " — ну они вынули по копейке и дали ему батальон гусаров и на которой и не мог узнать от солдат \n",
      " ему все сначала свое положение \n",
      " — а рубашка — вскрикнул граф открывая мокрые глаза и сложив сморщенные старые руки на голову и развел руками \n",
      " но я рад очень и желаю чтоб она ехала в карете подъезжает к дому \n",
      " беклешов и федор петрович уваров приехавшие с ним спелись дорогой \n",
      " посланный офицер для\n",
      "\n",
      "\n",
      "— сказал полковой командир покраснел подбежал к старшему офицеру и весело захохотал что денисову и долохову тоже партизану с небольшой партией ходившему близко от нее требовали то есть люди которые веруют в то время как он хотел поцеловаться \n",
      " сделайте милость пожалуйста оставьте \n",
      " его новые братья дали ему письма в петербург и про свояченицу \n",
      " вблизи весело блестел купол ново-девичьего монастыря увидал морозную росу на пыльной траве увидал холмы воробьевых гор и извивающийся над рекою и скрывающийся в лиловой шляпке передала ему пакет \n",
      " после многих советов и переговоров граф придумал наконец средство для успокоения отдавал жертву народу и\n",
      "\n",
      "\n",
      "\n",
      " — он не думал что у них — это он встал \n",
      " — давно у тебя выйдет с тремя офицерами но жерков отталкивая рукой несвицкого запыхавшимся голосом кричал он хотя не было \n",
      " стоит только отрешиться от установившегося в угоду матери забыть вас и вашу превосходную мать \n",
      " как только князь андрей встал и подойдя к нему его жены \n",
      " давая и принимая участие в нем самом — он подумает что у вас все эти кутежи и под влиянием вина пьер следил за движениями француза \n",
      " — да это все равно \n",
      " ростов продолжая оглядываться на огни и шуметь\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(5):\n",
    "    print(generate(matrix_corpus, id2unigram, bigram2id))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9b5fc",
   "metadata": {},
   "source": [
    "Теперь рассчитаем перплексию модели на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea31d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(probas):\n",
    "    p = np.exp(np.sum(probas))\n",
    "    N = len(probas)\n",
    "    return p**(-1/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d555f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "for sent in y:\n",
    "    \n",
    "    probs = []\n",
    "    for trigram in ngrammer(sent, 3):\n",
    "        word1, word2, word3 = trigram.split()\n",
    "        bigram = word1 + ' ' + word2\n",
    "        \n",
    "        if bigram in bigrams and trigram in trigrams:\n",
    "            probs.append(np.log(trigrams[trigram]/bigrams[bigram]))\n",
    "        else:\n",
    "            probs.append(np.log(0.00001))\n",
    "    \n",
    "    metrics.append(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "169e252e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12907.601228660009\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([perplexity(x) for x in metrics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?\n",
    "\n",
    "Существует еще как минимум два способа решения проблемы несловарных слов.<br><br>\n",
    "Первый из них применятся для систем с открытым словарем (то есть таких, в которых словарь задается вне обучающей выборки, поэтому в тренировочном сете появляются несловарные слова). Каждому слову в обучающей выборке, которое не встретилось в словаре, присваивается специальный токен \\<OOV> (out of vocabulary) или \\<UNK> (unknown), а дальше вероятности для этого токена рассчитываются также, как и для остальных слов в датасете.<br><br>\n",
    "Второй способ применяется в ситуациях, когда словарь изначально не задан и собирается на основе обучающего сета. Чтобы словарь был не слишком объемный, можно избавиться от очень редких слов (можно задать, например, значение частотности, по которому будут отбираться слова, или заранее задать размерность словаря). Для таких слов так же, как и при первом способе, задается специальный токен \\<OOV> или \\<UNK> и для него считается вероятность."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?\n",
    "\n",
    "Сглаживание - это способ решения проблемы редких слов, не встретившихся в обучающем корпусе. Те слова (н-граммы) из тестовой выборки, которых не было в тренировочном датасете, в процессе обучения получают нулевую вероятность, хотя, конечно, не должны, так как все же в текстах встречаются. Чтобы предотвратить присваивание таким словам нулевой вероятности, существует несколько методов сглаживания. Самый простой способ - сглаживание Лапласа (это добавление 1 ко всем частотам в корпусе) или чуть расширенный метод - сглаживание add-k (это добавление не 1, а k, заданного от 0 до 1). Существуют также методы, суть которых сводится к тому, что вероятность не встретившихся н-грамм рассчитывается на основании (н-1)-грамм (Kneser-Ney smoothing)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
