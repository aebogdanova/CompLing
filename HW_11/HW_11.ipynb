{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 11. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOPchj_ddBtt",
        "outputId": "ed777999-8bda-4785-a878-86cbf6e78827"
      },
      "id": "hOPchj_ddBtt",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "b-NXmG79c95F"
      },
      "id": "b-NXmG79c95F",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачиваем корпуса из [TED2020](https://opus.nlpl.eu/TED2020.php) (возьмем языковую пару EN-FR)."
      ],
      "metadata": {
        "id": "2VHXaMMjbAba"
      },
      "id": "2VHXaMMjbAba"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "ROOT_DIR = '/content/drive' \n",
        "drive.mount(ROOT_DIR)"
      ],
      "metadata": {
        "id": "TplgKdkZbsbg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525d2eeb-5447-4471-d292-128936d356da"
      },
      "id": "TplgKdkZbsbg",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P drive/MyDrive/HSE_compling/HW_11 https://opus.nlpl.eu/download.php?f=TED2020/v1/moses/en-fr.txt.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZx4bDCFmUKJ",
        "outputId": "6263b9b9-0c20-447f-de80-b821883be3bd"
      },
      "id": "YZx4bDCFmUKJ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-22 16:32:27--  https://opus.nlpl.eu/download.php?f=TED2020/v1/moses/en-fr.txt.zip\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-fr.txt.zip [following]\n",
            "--2022-05-22 16:32:28--  https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-fr.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30006604 (29M) [application/zip]\n",
            "Saving to: ‘drive/MyDrive/HSE_compling/HW_11/download.php?f=TED2020%2Fv1%2Fmoses%2Fen-fr.txt.zip’\n",
            "\n",
            "download.php?f=TED2 100%[===================>]  28.62M  11.0MB/s    in 2.6s    \n",
            "\n",
            "2022-05-22 16:32:32 (11.0 MB/s) - ‘drive/MyDrive/HSE_compling/HW_11/download.php?f=TED2020%2Fv1%2Fmoses%2Fen-fr.txt.zip’ saved [30006604/30006604]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip drive/MyDrive/HSE_compling/HW_11/download.php?f=TED2020%2Fv1%2Fmoses%2Fen-fr.txt.zip -d drive/MyDrive/HSE_compling/HW_11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYjgPebQmg-R",
        "outputId": "ea167850-e995-49b3-811d-96756ca18aca"
      },
      "id": "MYjgPebQmg-R",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  drive/MyDrive/HSE_compling/HW_11/download.php?f=TED2020%2Fv1%2Fmoses%2Fen-fr.txt.zip\n",
            "  inflating: drive/MyDrive/HSE_compling/HW_11/README  \n",
            "  inflating: drive/MyDrive/HSE_compling/HW_11/LICENSE  \n",
            "  inflating: drive/MyDrive/HSE_compling/HW_11/TED2020.en-fr.en  \n",
            "  inflating: drive/MyDrive/HSE_compling/HW_11/TED2020.en-fr.fr  \n",
            "  inflating: drive/MyDrive/HSE_compling/HW_11/TED2020.en-fr.xml  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Откроем датасет и посмотрим на примеры."
      ],
      "metadata": {
        "id": "JjdFRLJabvvM"
      },
      "id": "JjdFRLJabvvM"
    },
    {
      "cell_type": "code",
      "source": [
        "project_dir = 'drive/MyDrive/HSE_compling/HW_11/'"
      ],
      "metadata": {
        "id": "LTmHutvb57Jf"
      },
      "id": "LTmHutvb57Jf",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents_all = open(f'{project_dir}TED2020.en-fr.en').read().lower().splitlines()\n",
        "fr_sents_all = open(f'{project_dir}TED2020.en-fr.fr').read().lower().splitlines()\n",
        "\n",
        "print(en_sents_all[5], fr_sents_all[5])\n",
        "print(en_sents_all[-5], fr_sents_all[-5])\n",
        "\n",
        "print(len(en_sents_all))"
      ],
      "metadata": {
        "id": "GWGgllbNQsSQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6ea6e0-90fd-43b5-bb81-81e9745c66b6"
      },
      "id": "GWGgllbNQsSQ",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(laughter) i flew on air force two for eight years.  j'ai volé avec air force 2 pendant huit ans. \n",
            "today, we stand ready to tell our own stories without compromise, without apologies.  aujourd'hui, nous sommes prêts à raconter notre propre histoire, sans faire de compromis, sans s'excuser. \n",
            "410443\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сразу отложим тестовую выборку."
      ],
      "metadata": {
        "id": "vqoZX-MSKovR"
      },
      "id": "vqoZX-MSKovR"
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents, en_sents_test, fr_sents, fr_sents_test = train_test_split(en_sents_all, fr_sents_all, test_size=0.05)\n",
        "\n",
        "print(len(en_sents), len(en_sents_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpYyT9ylKLNP",
        "outputId": "e17ad447-c7ae-44f3-d362-089039c0a1db"
      },
      "id": "GpYyT9ylKLNP",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "389920 20523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для токенизации используем WordPiece Tokenizer из библиотеки tokenizers от HG. В токенизатор передаем специальные токены: UNK для неизвестных слов, CLS для начала текста, SEP для конца текста, PAD для паддинга."
      ],
      "metadata": {
        "id": "1Gd1FASmdbPU"
      },
      "id": "1Gd1FASmdbPU"
    },
    {
      "cell_type": "code",
      "source": [
        "# токенизация для английского\n",
        "tokenizer_en = Tokenizer(WordPiece(), )\n",
        "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_en = WordPieceTrainer(vocab_size=30000, special_tokens=['[UNK]', '[CLS]', '[SEP]', '[PAD]'])\n",
        "tokenizer_en.train(files=[f'{project_dir}TED2020.en-fr.en'], trainer=trainer_en)\n",
        "\n",
        "# токенизация для французского\n",
        "tokenizer_fr = Tokenizer(WordPiece(), )\n",
        "tokenizer_fr.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_fr.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_fr = WordPieceTrainer(vocab_size=30000, special_tokens=['[UNK]', '[CLS]', '[SEP]', '[PAD]'])\n",
        "tokenizer_fr.train(files=[f'{project_dir}TED2020.en-fr.fr'], trainer=trainer_fr)"
      ],
      "metadata": {
        "id": "EIWQPhHocwyz"
      },
      "id": "EIWQPhHocwyz",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# декодеры из decoders (библиотеки tokenizers)\n",
        "tokenizer_en.decoder = decoders.WordPiece()\n",
        "tokenizer_fr.decoder = decoders.WordPiece()"
      ],
      "metadata": {
        "id": "ygEzqimxepfE"
      },
      "id": "ygEzqimxepfE",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соохраним токенизаторы в отдельные файлы, чтобы не потерять их."
      ],
      "metadata": {
        "id": "GtWs8omBfWE6"
      },
      "id": "GtWs8omBfWE6"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en.save(f'{project_dir}tokenizer_en')\n",
        "tokenizer_fr.save(f'{project_dir}tokenizer_fr')\n",
        "\n",
        "tokenizer_en = Tokenizer.from_file(f'{project_dir}tokenizer_en')\n",
        "tokenizer_fr = Tokenizer.from_file(f'{project_dir}tokenizer_fr')"
      ],
      "metadata": {
        "id": "6DuJeErVfN46"
      },
      "id": "6DuJeErVfN46",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Переводим тексты в последовательности индексов (в начало добавляется токен CLS, в конец -- токен SEP)."
      ],
      "metadata": {
        "id": "3F-icTxqhErq"
      },
      "id": "3F-icTxqhErq"
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, target=False):\n",
        "    return [tokenizer.token_to_id('[CLS]')] + tokenizer.encode(text).ids + [tokenizer.token_to_id('[SEP]')]\n",
        "\n",
        "X_en = [encode(text, tokenizer_en) for text in en_sents]\n",
        "X_fr = [encode(text, tokenizer_fr, target=True) for text in fr_sents]"
      ],
      "metadata": {
        "id": "XJ4qdWWnfsk8"
      },
      "id": "XJ4qdWWnfsk8",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на среднюю длину последовательностей в обоих корпусах."
      ],
      "metadata": {
        "id": "ArG8NWXsha0i"
      },
      "id": "ArG8NWXsha0i"
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en = np.mean([len(x) for x in X_en])\n",
        "max_len_fr = np.mean([len(x) for x in X_fr])\n",
        "\n",
        "print(max_len_en, max_len_fr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEX6OPOdgE7d",
        "outputId": "218b0dcf-31c4-4a0e-c428-3b5ec4bcd437"
      },
      "id": "sEX6OPOdgE7d",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.84078528929011 24.870665777595406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оставим в качестве значений max_len_en и max_len_fr значения 22 и 24 соответственно."
      ],
      "metadata": {
        "id": "0vX3TWuChnhN"
      },
      "id": "0vX3TWuChnhN"
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en, max_len_fr = 22, 24"
      ],
      "metadata": {
        "id": "n4iygq4Pg91A"
      },
      "id": "n4iygq4Pg91A",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer_en.token_to_id('[PAD]')"
      ],
      "metadata": {
        "id": "k6iBEGB2Vrjn"
      },
      "id": "k6iBEGB2Vrjn",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      X_en, maxlen=max_len_en, padding='post', value=PAD_IDX)\n",
        "\n",
        "X_fr_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      [x[1:] for x in X_fr], maxlen=max_len_fr-1, padding='post', value=PAD_IDX)\n",
        "\n",
        "X_fr_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      [x[:-1] for x in X_fr], maxlen=max_len_fr-1, padding='post', value=PAD_IDX)\n",
        "\n",
        "X_en.shape, X_fr_out.shape, X_fr_dec.shape"
      ],
      "metadata": {
        "id": "Kdfk97WGiWPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2f4a48-0f35-452d-f13c-ffef2efd11fa"
      },
      "id": "Kdfk97WGiWPB",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((389920, 22), (389920, 23), (389920, 23))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_en_train, X_en_valid, X_fr_dec_train, X_fr_dec_valid, X_fr_out_train, X_fr_out_valid = train_test_split(X_en, X_fr_dec, X_fr_out, test_size=0.1)"
      ],
      "metadata": {
        "id": "3a5MbnVVkGB-"
      },
      "id": "3a5MbnVVkGB-",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение модели."
      ],
      "metadata": {
        "id": "4Cd-kcgvUibk"
      },
      "id": "4Cd-kcgvUibk"
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    \"\"\"Calculate the attention weights. \"\"\"\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # add the mask to zero out padding tokens\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k)\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "g0P6XKliT4__"
      },
      "id": "g0P6XKliT4__",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name='multi_head_attention'):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # linear layers\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # split heads\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # scaled dot-product attention\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # concatenation of heads\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # final linear layer\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "JcxknDwvUk80"
      },
      "id": "JcxknDwvUk80",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :] "
      ],
      "metadata": {
        "id": "iItlb-BsUoMo"
      },
      "id": "iItlb-BsUoMo",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "YPem33sQUt54"
      },
      "id": "YPem33sQUt54",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "AVN-gzeoUwwu"
      },
      "id": "AVN-gzeoUwwu",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name='encoder_layer'):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name='inputs')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name='attention')({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "7e8jQFv6VHzM"
      },
      "id": "7e8jQFv6VHzM",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='encoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='encoder_layer_{}'.format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "nF0-aQ2OVKNp"
      },
      "id": "nF0-aQ2OVKNp",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name='decoder_layer'):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name='attention_1')(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "    attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name='attention_2')(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "nBn8IZScVMl4"
      },
      "id": "nBn8IZScVMl4",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "1gPQdwdpVO4Y"
      },
      "id": "1gPQdwdpVO4Y",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                max_len,\n",
        "                name='transformer'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name='dec_inputs')\n",
        "\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "    # mask the future tokens for decoder inputs at the 1st attention block\n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "    # mask the encoder outputs for the 2nd attention block\n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "      vocab_size=vocab_size[0],\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      max_len=max_len[0],\n",
        "    )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "      vocab_size=vocab_size[1],\n",
        "      num_layers=num_layers,\n",
        "      units=units,\n",
        "      d_model=d_model,\n",
        "      num_heads=num_heads,\n",
        "      dropout=dropout,\n",
        "      max_len=max_len[1],\n",
        "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name='outputs')(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "X9-C1yQ1VQ4O"
      },
      "id": "X9-C1yQ1VQ4O",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = L(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "tmpPf-yoVUwF"
      },
      "id": "tmpPf-yoVUwF",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "3-AqAHYsVzre"
      },
      "id": "3-AqAHYsVzre",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "with mirrored_strategy.scope():\n",
        "    model = transformer(\n",
        "        vocab_size=(tokenizer_en.get_vocab_size(), tokenizer_fr.get_vocab_size()),\n",
        "        num_layers=NUM_LAYERS,\n",
        "        units=UNITS,\n",
        "        d_model=D_MODEL,\n",
        "        num_heads=NUM_HEADS,\n",
        "        dropout=DROPOUT,\n",
        "        max_len=[max_len_en, max_len_fr])\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "        0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "    def accuracy(y_true, y_pred):\n",
        "        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{project_dir}model_enfr',\n",
        "                                                monitor='val_loss',\n",
        "                                                verbose=1,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                mode='min',\n",
        "                                                save_freq='epoch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D02yr__V5ah",
        "outputId": "8588d961-4d3e-4782-dca5-05539b2d87fb"
      },
      "id": "7D02yr__V5ah",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit((X_en_train, X_fr_dec_train), X_fr_out_train, \n",
        "             validation_data=((X_en_valid, X_fr_dec_valid), X_fr_out_valid),\n",
        "             batch_size=200,\n",
        "             epochs=100,\n",
        "             callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 832
        },
        "id": "cTvm-kc8WQsI",
        "outputId": "5a9dc93e-86b0-4ca0-ce58-ddded58bc115"
      },
      "id": "cTvm-kc8WQsI",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "1755/1755 [==============================] - ETA: 0s - loss: 2.7215 - accuracy: 0.3316\n",
            "Epoch 1: val_loss improved from inf to 1.87981, saving model to drive/MyDrive/HSE_compling/HW_11/model_enfr\n",
            "1755/1755 [==============================] - 364s 199ms/step - loss: 2.7215 - accuracy: 0.3316 - val_loss: 1.8798 - val_accuracy: 0.4378\n",
            "Epoch 2/100\n",
            "1755/1755 [==============================] - ETA: 0s - loss: 1.7250 - accuracy: 0.4510\n",
            "Epoch 2: val_loss improved from 1.87981 to 1.58950, saving model to drive/MyDrive/HSE_compling/HW_11/model_enfr\n",
            "1755/1755 [==============================] - 356s 203ms/step - loss: 1.7250 - accuracy: 0.4510 - val_loss: 1.5895 - val_accuracy: 0.4726\n",
            "Epoch 3/100\n",
            "1755/1755 [==============================] - ETA: 0s - loss: 1.4959 - accuracy: 0.4786\n",
            "Epoch 3: val_loss improved from 1.58950 to 1.49366, saving model to drive/MyDrive/HSE_compling/HW_11/model_enfr\n",
            "1755/1755 [==============================] - 356s 203ms/step - loss: 1.4959 - accuracy: 0.4786 - val_loss: 1.4937 - val_accuracy: 0.4851\n",
            "Epoch 4/100\n",
            "1755/1755 [==============================] - ETA: 0s - loss: 1.3818 - accuracy: 0.4928\n",
            "Epoch 4: val_loss improved from 1.49366 to 1.44258, saving model to drive/MyDrive/HSE_compling/HW_11/model_enfr\n",
            "1755/1755 [==============================] - 355s 203ms/step - loss: 1.3818 - accuracy: 0.4928 - val_loss: 1.4426 - val_accuracy: 0.4927\n",
            "Epoch 5/100\n",
            "1755/1755 [==============================] - ETA: 0s - loss: 1.3085 - accuracy: 0.5023\n",
            "Epoch 5: val_loss improved from 1.44258 to 1.41542, saving model to drive/MyDrive/HSE_compling/HW_11/model_enfr\n",
            "1755/1755 [==============================] - 355s 202ms/step - loss: 1.3085 - accuracy: 0.5023 - val_loss: 1.4154 - val_accuracy: 0.4963\n",
            "Epoch 6/100\n",
            "  10/1755 [..............................] - ETA: 5:34 - loss: 1.1643 - accuracy: 0.5221"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-3f78e648f585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m              callbacks=[checkpoint])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для перевода текста (с семинара)."
      ],
      "metadata": {
        "id": "4sMyluToF6Tk"
      },
      "id": "4sMyluToF6Tk"
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    input_ids = encode(text.lower(), tokenizer_en)\n",
        "\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences([input_ids], maxlen=max_len_en, padding='post', value=PAD_IDX)\n",
        "\n",
        "    output_ids = [tokenizer_fr.token_to_id('[CLS]')]\n",
        "    \n",
        "    pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False)\n",
        " \n",
        "    while pred.numpy().argmax(2)[0][-1] not in [tokenizer_fr.token_to_id('[SEP]')]:\n",
        "        if len(output_ids) > max_len_fr:\n",
        "            break\n",
        "        output_ids.append(pred.numpy().argmax(2)[0][-1])\n",
        "        pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False)\n",
        "\n",
        "    return tokenizer_fr.decode(output_ids[1:])"
      ],
      "metadata": {
        "id": "5FHoPqxNzloF"
      },
      "id": "5FHoPqxNzloF",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Oh my God! They killed Kenny!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "z_Ff6H4gzllt",
        "outputId": "b7ecc884-6911-4f87-dae9-66f11f1992f7"
      },
      "id": "z_Ff6H4gzllt",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oh mon dieu! ils ont tué kenny!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('Machine translation is the use of computers to translate from one language to another.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "i3tfNr6a0g16",
        "outputId": "96bc1546-67a7-473d-e295-c3dc13952a7a"
      },
      "id": "i3tfNr6a0g16",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"la traduction automatique est l'utilisation des ordinateurs pour traduire d'une langue à l'autre.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для перевода текстов батчами."
      ],
      "metadata": {
        "id": "mkssJFvbGFTE"
      },
      "id": "mkssJFvbGFTE"
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_batch(texts):\n",
        "\n",
        "    input_ids = [encode(text.lower(), tokenizer_en) for text in texts]\n",
        "\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                        input_ids, \n",
        "                        maxlen=max_len_en, \n",
        "                        padding='post', \n",
        "                        value=PAD_IDX)\n",
        "\n",
        "    output_ids = [[tokenizer_fr.token_to_id('[CLS]')] for i in range(len(texts))]\n",
        "\n",
        "    pred = model((input_ids, tf.cast(output_ids, tf.int32)), training=False)\n",
        "\n",
        "    SEP_IDX = tokenizer_fr.token_to_id('[SEP]')\n",
        "\n",
        "    # проходимся по output_ids столько раз, сколько позволяет максимальная длина последовательностей (max_len_fr)\n",
        "    for i in range(max_len_fr-1):\n",
        "        # проходимся по каждой последовательности\n",
        "        for i in range(len(output_ids)):\n",
        "            # если последний индекс в предложении - индекс паддинга или если следующий предсказываемый индекс - индекс окончания предложения,\n",
        "            # то добавляем в последовательность индекс паддинга\n",
        "            if output_ids[i][-1] == PAD_IDX or pred.numpy().argmax(2)[i][-1] == SEP_IDX:\n",
        "                output_ids[i].append(PAD_IDX)\n",
        "            # в противном случае добавляем следующий предсказываемый моделью индекс\n",
        "            else:\n",
        "                output_ids[i].append(pred.numpy().argmax(2)[i][-1])\n",
        "        # делаем следующее предсказание\n",
        "        pred = model((input_ids, tf.cast(output_ids, tf.int32)), training=False)\n",
        "\n",
        "    translated = []\n",
        "    # декодируем каждую последовательность в output_ids\n",
        "    for output in output_ids:\n",
        "        # из последовательности для декодирования оставляем все индексы, кроме 1 (индекса начала предложения) и индексов паддинга\n",
        "        seq = [idx for idx in output if (idx != PAD_IDX) & (idx != 1)]\n",
        "        translated.append(tokenizer_fr.decode(seq))\n",
        "\n",
        "    return translated"
      ],
      "metadata": {
        "id": "suxizkjz1B5k"
      },
      "id": "suxizkjz1B5k",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_batch(['Oh my God! They killed Kenny!', 'Machine translation is the use of computers to translate from one language to another.', 'London is the capital of Great Britain.'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LXTa5kk1qG5",
        "outputId": "b28b4094-3043-495c-8703-3f154c38ab74"
      },
      "id": "4LXTa5kk1qG5",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['oh mon dieu! ils ont tué kenny!',\n",
              " \"la traduction automatique est l'utilisation des ordinateurs pour traduire d'une langue à l'autre.\",\n",
              " 'londres est le capital de grande - bretagne.']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценим модель на тестовой выборке."
      ],
      "metadata": {
        "id": "6t-hInSwJj9s"
      },
      "id": "6t-hInSwJj9s"
    },
    {
      "cell_type": "code",
      "source": [
        "print(en_sents_test[0])\n",
        "print(fr_sents_test[0])"
      ],
      "metadata": {
        "id": "Q15aOXayMxI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb608a9c-b0e4-4da0-e75b-988a77c98477"
      },
      "id": "Q15aOXayMxI6",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "435 representatives, plus 100 senators, and 3 electors from the district of columbia. \n",
            "435 représentants, 100 sénateurs et 3 électeurs du district de columbia. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "f-AgRE5KUExp"
      },
      "id": "f-AgRE5KUExp",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# разделим выборку на батчи\n",
        "def split_batches(sents, batch_size):\n",
        "    for i in range(0, len(sents), batch_size):\n",
        "        yield sents[i:i+batch_size]\n",
        "\n",
        "en_batches = list(split_batches(en_sents_test, 16))\n",
        "fr_batches = list(split_batches(fr_sents_test, 16))"
      ],
      "metadata": {
        "id": "1Yy-yT8NJf3B"
      },
      "id": "1Yy-yT8NJf3B",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleus = []\n",
        "\n",
        "for i_b, batch in tqdm.tqdm(enumerate(en_batches)):\n",
        "    translations = translate_batch(batch)\n",
        "    for i_t, translation in enumerate(translations):\n",
        "          reference = tokenizer_fr.encode(translation).tokens\n",
        "          hypothesis = tokenizer_fr.encode(fr_batches[i_b][i_t]).tokens\n",
        "          bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis))"
      ],
      "metadata": {
        "id": "AO_yS8QEQ4MK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "ed8c9196-6bcb-40a3-8c0e-9d0682834de4"
      },
      "id": "AO_yS8QEQ4MK",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "414it [44:02,  6.38s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-5868e1de750a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_fr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-b87b8e81354f>\u001b[0m in \u001b[0;36mtranslate_batch\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# в противном случае добавляем следующий предсказываемый моделью индекс\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0moutput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# делаем следующее предсказание\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(bleus)/len(bleus))*100"
      ],
      "metadata": {
        "id": "UJkpnTCbUVEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d34655-4dbd-437c-a21d-07d22588bc89"
      },
      "id": "UJkpnTCbUVEe",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.239878522818"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Техника back translation -- это способ увеличить объем обучающих данных путем искусственного создания пар предложений на двух языках.  Back translation применяют в том случае, когда объема имеющегося параллельного корпуса недостаточно, текстов на языке-источнике мало, а текстов на целевом языке много. Суть техники состоит в том, что сначала на параллельном корпусе обучают модель-переводчик с целевого языка на язык-источник, затем с помощью этой модели переводят любые другие тексты на целевом языке на язык-источник, пополняя тем самым объем обучающей выборки для обучения модели-переводчика с языка-источника на целевой язык. \n",
        "\n",
        "Применение этой техники к паре en-ru на данных из семинара выглядело бы так:\n",
        "\n",
        "1) Обучение на имеющемся параллельном корпусе (в нашем случае - из корпуса opus-100) модели, осуществляющей перевод с русского на английский (то же самое, что было проделано на семинаре, только таргетным языком был бы английский);\n",
        "\n",
        "2) Генерация перевода какого-нибудь другого корпуса на русском языке (например, корпуса Википедии) с помощью данной модели на английский язык; сохранение полученных пар предложений в отдельный датасет;\n",
        "\n",
        "3) Обучение на полученном датасете MT-модели для перевода с английского языка на русский."
      ],
      "metadata": {
        "id": "RzLE9UPadrMW"
      },
      "id": "RzLE9UPadrMW"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "HW_11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}